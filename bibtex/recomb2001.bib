@inproceedings{10.1145/369133.369143,
author = {Adams, Mark},
title = {The sequence of the human genome (abstract only)},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369143},
doi = {10.1145/369133.369143},
abstract = {A consensus sequence of the euchromatic portion of human genome has been generated by the whole genome shot-gun sequencing method that was developed while sequencing the genomes of Haemophilus influenzae and Drosophila melanogaster. The 2.9 billion bp sequence, was generated over nine months from 27,271,853 high quality sequence reads (≁5X coverage of the genome) from both ends of plasmid clones made from the DNA of five individuals: three females and two males of African-American, Asian-Chinese, Hispanic and Caucasian ethnicity. The coverage of the genome in cloned DNA represented by paired end-sequences exceeds 37X. Two assembly methods, a whole genome assembly and a regional hybrid assembly were utilized, combining BAC data from GenBank with Celera data. Over 90 assemblies of 500,000 bp or greater and 2510 million bp or larger. Analysis of the genome sequence reveals - 26,178 protein-encoding genes for which there is strong corroborating evidence and an additional 12,000 computationally derived genes with mouse homologues or other weak supporting evidence. Comparative genomic analysis indicates vertebrate expansions of genes associated with neuronal function, tissue-specific developmental regulation, and in the hemostasis and immune systems. DNA sequence comparisons among the five individuals provided locations of 2.6 million single nucleotide polymorphisms (SNPs). The haploid genomes of a randomly drawn pair of humans differ at a rate of one per 1,250 bp on average but there is marked heterogeneity in the level of polymorphism across the genome. Only 0.75\% of the SNPs led to possibly dysfunctional proteins.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {1},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369146,
author = {Arslan, Abdullah N. and E\u{g}ecio\u{g}lu, \"{O}mer and Pevzner, Pavel A.},
title = {A new approach to sequence comparison: normalized sequence alignment},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369146},
doi = {10.1145/369133.369146},
abstract = {The Smith-Waterman algorithm for local sequence alignment is one of the most important techniques in computational molecular biology. This ingenious dynamic programming approach was designed to reveal the highly conserved fragments by discarding poorly conserved initial and terminal segments. However, the existing notion of local similarity has a serious flaw: it does not discard poorly conserved intermediate segments. The Smith-Waterman algorithm finds the local alignment with maximal score but it is unable to find local alignment with maximum degree of similarity (e.g., maximal percent of matches). Moreover, there is still no efficient algorithm that answers the following natural question: do two sequences share a (sufficiently long) fragment with more than 70\% of similarity? As a result, the local alignment sometimes produces a mosaic of well-conserved fragments artificially connected by poorly-conserved or even unrelated fragments. This may lead to problems in comparison of long genomic sequences and comparative gene prediction as recently pointed out by Zhang et al., 1999 [33]. In this paper we propose a new sequence comparison algorithm (normalized local alignment) that reports the regions with maximum degree of similarity. The algorithm is based on fractional programming and its running time is Ο(n2 log n). In practice, normalized local alignment is only 3-5 times slower than the standard Smith-Waterman algorithm.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {2–11},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369150,
author = {Barash, Yoseph and Friedman, Nir},
title = {Context-specific Bayesian clustering for gene expression data},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369150},
doi = {10.1145/369133.369150},
abstract = {The recent growth in genomic data and measurement of genome-wide expression patterns allows to examine gene regulation by transcription factors using computational tools. In this work, we present a class of mathematical models that help in understanding the connections between transcription factors and functional classes of genes based on genetic and genomic data. These models represent the joint distribution of transcription factor binding sites and of expression levels of a gene in a single model. Learning a combined probability model of binding sites and expression patterns enables us to improve the clustering of the genes based on the discovery of putative binding sites and to detect which binding sites and experiments best characterize a cluster. To learn such models from data, we introduce a new search method that rapidly learns a model according to a Bayesian score. We evaluate our method on synthetic data as well as on real data and analyze the biological insights it provides.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {12–21},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369152,
author = {Beigel, Richard and Alon, Noga and Kasif, Simon and Apaydin, Mehmet Serkan and Fortnow, Lance},
title = {An optimal procedure for gap closing in whole genome shotgun sequencing},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369152},
doi = {10.1145/369133.369152},
abstract = {Tettelin et. al. proposed a new method for closing the gaps in whole genome shotgun sequencing projects. The method uses a multiplex PCR strategy in order to minimize the time and effort required to sequence the DNA in the missing gaps. This procedure has been used in a number of microbial sequencing projects including Streptococcus pneumoniae and other bacteria. In this paper we describe a theoretical framework for this problem and propose an improved method that guarantees to minimize the number of steps involved in the gap closure procedures. In given particular collection of n/2 DNA fragments we describe a strategy that requires. 0.75 log n work in eight parallel rounds of experiment closely matching a corresponding lower bound 0.5 log of n},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {22–30},
numpages = {9},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369167,
author = {Ben-Dor, Amir and Friedman, Nir and Yakhini, Zohar},
title = {Class discovery in gene expression data},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369167},
doi = {10.1145/369133.369167},
abstract = {Recent studies (Alizadeh et al, [1]; Bittner et al,[5]; Golub et al, [11]) demonstrate the discovery of putative disease subtypes from gene expression data. The underlying computational problem is to partition the set of sample tissues into statistically meaningful classes. In this paper we present a novel approach to class discovery and develop automatic analysis methods. Our approach is based on statistically scoring candidate partitions according to the overabundance of genes that separate the different classes. Indeed, in biological datasets, an overabundance of genes separating known classes is typically observed. we measure overabundance against a stochastic null model. This allows for highlighting subtle, yet meaningful, partitions that are supported on a small subset of the genes.Using simulated annealing we explore the space of all possible partitions of the set of samples, seeking partitions with statistically significant overabundance of differentially expressed genes. We demonstrate the performance of our methods on synthetic data, where we recover planted partitions. Finally, we turn to tumor expression datasets, and show that we find several highly pronounced partitions.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {31–38},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369168,
author = {Bilu, Yonatan and Linial, Michal},
title = {On the predictive power of sequence similarity in yeast},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369168},
doi = {10.1145/369133.369168},
abstract = {Perhaps the most direct way to infer functional linkage of proteins is through structural similarity. However, structure determination lags behind DNA sequencing. Here we show that sequence similarity based on nucleotide sequences alone between ORFs in yeast is indicative of the corresponding genes being of the same functional group, having a similar gene expression pattern or of being involved in a protein-protein interaction. In particular, we compare the nucleotide sequences corresponding to the 6280 yeast ORFs using BLAST, and then cluster them together using a simple neighbor-joining algorithm. This, in effect, gives us hierarchical clustering of 53 levels, where higher levels have bigger clusters. We compare the clustering to large databases that are not based a-priori on sequence information to get a notion of how well our clustering is correlated with this data. For functional annotation we use the SGD database that gives one of 540 annotations for about half the yeast genes. For all pairs that appear within a cluster, we test the hypothesis that almost all genes within the same cluster have the same function. We get very high percentage rates of correct annotation at the lower levels of the hierarchy, which decreases gradually at higher ones. From the results of the large scale gene expression experiments we generate a list of pairs of genes, whose expression is highly correlated (≱0.9). We then go over this list of pairs, and count how many of them are contained within a cluster as opposed to the expected number by a random model. We estimate the significance of our results using simulation, and get for all levels p-value≰≰0.001. The third type of data obtained from the protein-protein interaction database that is given as a list of airs of proteins involved in interaction.As before, we count how many pairs are contained within a cluster, and get much better results than expected by a random model, with p-values≰0.001 for almost all levels. In summary, we show that successful functional predictions and functional annotations can be applied at a genomic scale. This can be achieved by combining a naive hierarchical clustering method that creates sets of clusters at different levels of granularity with statistical validation tools.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {39–48},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369170,
author = {Blanchette, Mathieu},
title = {Algorithms for phylogenetic footprinting},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369170},
doi = {10.1145/369133.369170},
abstract = {Phylogenetic footprinting is a technique that identifies regulatory elements by finding unusually well conserved regions in a set of orthologous non-coding DNA sequences from multiple species. In an earlier paper, we presented an exact algorithm that identifies the most conserved region of a set of sequences. Here, we present a number of algorithmic improvements that produce a 1000 fold speedup over the original algorithm. We also show how prior knowledge can be used to identify weaker motifs, and how to handle data sets in which only an unknown subset of the sequences contain the regulatory element. Each technique is implemented and successfully identifies a large number of known binding sites, as well as several highly conserved but uncharacterized regions.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {49–58},
numpages = {10},
keywords = {algorithm, motif finding, phylogenetic footprinting, phylogeny, regulatory elements},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369171,
author = {Bradley, Phil and Cowen, Lenore and Menke, Matthew and King, Jonathan and Berger, Bonnie},
title = {Predicting the β-helix fold from protein sequence data},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369171},
doi = {10.1145/369133.369171},
abstract = {A method is presented that uses β-strand interactions to predict the right-handed β-helix super-secondary structural motif in protein sequences. A program called BetaWrap implements this method, and is shown to score known β-helices above non-β-helices in the Protein Data Bank in cross-validation. It is demonstrated that BetaWrap learns each of the seven known SCOP β-helix families, when trained on the the known β-helices from outside the family. BetaWrap also predicts many bacterial proteins of unknown structure that play a role in human infectious disease to β-helices; in particular, these proteins serve as virulence factors, adhesins and toxins in bacterial pathogenesis, and include cell surface proteins from Chlamydia and the intestinal bacterium Helicobacter pylori. The computational method used here may generalize to other β structures for which strand topology and profiles of residue accessibility are well conserved.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {59–67},
numpages = {9},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369175,
author = {Brent, Roger},
title = {Information processing by cells and biologists (abstract only)},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369175},
doi = {10.1145/369133.369175},
abstract = {The core agenda of post-WWII molecular biology has been defined as the molecular understanding of how genetic information was transmitted and read out (see for example Stent 1968), and, by the 1950's, the analogy between the tape in a Turing machine and the linear sequence of nucleotides in DNA was apparent to both computer scientists and biologists.In the early 21st century, it may be that molecular biology can fruitfully return to these roots, by recasting part of its agenda in terms of the need to understand how biological information is processed. In a somewhat more modern formulation, cells can be though of as machines that process and make decisions on three kinds of information: 1) information stored in the genome 2) information about intracellular events (for example from heckpoint mechanisms) and 3) information external to the cell.In many cases the machinery that cells use to make decisions is reasonably well understood at a qualitative level. However, in no case do we possess a corresponding quantitative understanding, and, reflecting this, nor are we very capable of predicting the outcomes of perturbations to the genome, the internal workings of the cell, or its external environment.One path to understanding the behavior of these ensembles of components clearly lies in construction of mechanism-based quantitative models representing cellular processes. Building such models requires solution of numerous computational and experimental biological challenges. I will detail some of these.Another path may involve computation on the qualitative biological knowledge that now exists. Expert biologists reason on this qualitative information to make statements about the consequences of perturbations, but expert systems that do the same in the main do not exist. Here, although the need is clear, the relative opacity (to me) of much of the seemingly relevant computer science literature has made it more difficult to figure out first steps.Finally, note that information theory (Shannon 1948) has its roots in the 20th century need to understand transmission of electrical signals through channels. It is not immediately clear that the representations of biological processes used by biologists map well to concepts that come from this theory. To give only one example, one is hard pressed to define or find, inside a cell that is processing signals from the outside, either the signal or the “bits” (Tukey, 1946) that might make it up. There may be thus be an opportunity here for new theory to guide thinking and further experiment.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {68},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369172,
author = {Buhler, Jeremy and Tompa, Martin},
title = {Finding motifs using random projections},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369172},
doi = {10.1145/369133.369172},
abstract = {Pevzner and Sze [23] considered a precise version of the motif discovery problem and simultaneously issued an algorithmic challenge: find a motif M of length 15, where each planted instance differs from M in 4 positions. Whereas previous algorithms all failed to solve this (15,4)-motif problem. Pevzner and Sze introduced algorithms that succeeded. However, their algorithms failed to solve the considerably more difficult (14,4)-, (16,5)-, and (18,6)-motif problems.We introduce a novel motif discovery algorithm based on the use of random projections of the input's substrings. Experiments on simulated data demonstrate that this algorithm performs better than existing algorithms and, in particular, typically solves the difficult (14,4)-, (16,5)-, and (18,6)-motif problems quite efficiently. A probabilistic estimate shows that the small values of d for which the algorithm fails to recover the planted (l, d)-motif are in all likelihood inherently impossible to solve. We also present experimental results on realistic biological data by identifying ribosome binding sites in prokaryotes as well as a number of known transcriptional regulatory motifs in eukaryotes.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {69–76},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369173,
author = {Bundschuh, Ralf},
title = {Rapid significance estimation in local sequence alignment with gaps},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369173},
doi = {10.1145/369133.369173},
abstract = {In order to assess the significance of sequence alignments it is crucial to know the distribution of alignment scores of pairs of random sequences. For gapped local alignment it is empirically known that the shape of this distribution is of the Gumbel form. However, the determination of the parameters of this distribution is a computationally very expensive task. We present a new algorithmic approach which allows to estimate the more important of the Gumbel parameters at least five times faster than the traditional methods. Actual runtimes of our algorithm between less than a second and a few minutes on a workstation bring significance estimation into the realm of interactive applications.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {77–85},
numpages = {9},
keywords = {Gumbel distribution, importance sampling, sequence alignment, statistical significance},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369174,
author = {Bussemaker, Harmen J. and Li, Hao and Siggia, Eric D.},
title = {Regulatory element detection using correlation with expression (abstract only)},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369174},
doi = {10.1145/369133.369174},
abstract = {We present a new computational method for discovering cis- regulatory elements which circumvents the need to cluster genes based on their expression profiles. Based on a model in which upstream motifs contribute additively to the expression level of a gene, it requires a single genome-wide set of expression ratios and the upstream sequence for each gene, and outputs statistically significant motifs. Analysis of publicly available expression data for Sac charomyes cerevisiae reveals several new putative regulatory elements, some of which plausibly control the early, transient induction of genes during sporulation. Known motifs generally have high statistical significance.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {86},
keywords = {cis-acting regulatory elements, linear least squares fit, microarrys, yeast},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369176,
author = {Chen, Ting},
title = {Gene-finding via tandem mass spectrometry},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369176},
doi = {10.1145/369133.369176},
abstract = {We propose a new gene-finding methodology that combines high performance liquid chromatograph (HPLC)-tandem mass spectrometry experiments with a fast computer algorithm to locate coding regions and introns. Proteins are first extracted from cells and digested by enzymes, and then the resulting peptides are separated and analyzed by HPLC-tandem mass spectrometry. We designed an algorithm to find DNA coding sequences, corresponding to open reading frames (ORF), in the genome such that their translated amino acid sequences are optimally correlated with these tandem mass spectra. In this algorithm, we also allow one gap, corresponding to an intron, between two DNA coding sequences, such that their concatenation becomes one coding sequence. Finally, the algorithm assembles these candidate coding sequences and introns into gene structures. Our algorithm was implemented to predict genes on 4 contigs with a total of 123 kbps using two sets of simulated digestion- HPLC-tandem mass spectrometry data of 2523 Caenorhabditis elegans Chromosome IV proteins, digested by trypsin and Asp-N respectively. Among 15 annotated genes in the forward strand, all 98 exons are hit by the predicted no-gap coding sequences, and 60 out of 83 introns are correctly predicted. We also tested gene structure prediction in a contig containing 3 genes. Combining splicing site predictions with predicted coding sequences and introns, we found all 3 gene structures.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {87–94},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369177,
author = {Chen, Ting and Jaffe, Jacob D. and Church, George M.},
title = {Algorithms for identifying protein cross-links via tandem mass spectrometry},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369177},
doi = {10.1145/369133.369177},
abstract = {Cross-linking technology combined with tandem mass spectrometry (MS-MS) is a powerful method that provides a rapid solution to the discovery of protein-protein interactions and protein structures. We studied the problem of detecting cross-linked peptides and cross-linked amino acids from tandem mass spectral data. Our method consists of two steps: the first step finds two protein subsequences whose mass sum equals a given mass measured from the mass spectrometry; and the second step finds the best cross-linked amino acids in these two peptide sequences that are optimally correlated to a given tandem mass spectrum. We designed fast and space-efficient algorithms for these two steps, and implemented and tested them on experimental data of cross-linked Hemoglobin proteins.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {95–102},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369179,
author = {Church, George},
title = {Hunger for new technologies, metrics, and spatiotemporal models in functional genomic (abstract only)},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369179},
doi = {10.1145/369133.369179},
abstract = {Functional genomics, as a field, is applying genomic self-improvement protocols (cost-effective, comprehensive, precise, accurate, and useful) to the kinetics of complex cellular systems. Radical surgery in functional biology aims to mimic the success of structural biology along all five of those axes. Technologies of recombinant DNA and automation have brought costs down exponentially (100-fold in ten years) in structural studies. That combined with definitions of completeness push the second axis (to better than 99.99 the third axis by the beautiful, brute force of repetition. To reduce systematic errors requires more finesse. Models allow integration of wildly different experimental methods (e.g. models based on the genetic code plus phylogeny provide quite independent checks of models based on DNA electrophoretic images). Model interchange specifications and metrics for model comparison mutually reinforce one another and provide one path along the fifth axis, that of utility, via killer-applications such as homology searches. This combination of modeling and searching provides serendipity and “functional hypothesis generation” in abundance. It instantly connects previously separately studied processes and organisms. Statistical assessment of agreement between experiment and calculation can lead to improvement of the types of model parameter as well as parameter values. What are the analogous metrics and models for functional genomics? How can we estimate possible lower limits to costs? How do we define completion and accuracy? Finally, how to we create and assess searches (not just on data but on models) and the utility of applications in general? How do these feed back to experimental design and feed forward to bioengineering?The functional genomics measures that are now thought to be prime for automation, miniaturization, and multiplexing include electrophoresis, molecular microarrays, mass-spectrometry, microscopy. Microscopy is well suited for non-destructive time series, measures concerning spatial effects and stochastic kinetics of systems of one or a few of any critical molecule. The other methods currently offer richer signatures for multiplex (measure many molecules from the same source atonce). Such extensive multiplexing can reduce errors due to misalignment of the (unmultiplexed) measures in space and/or time. These misalignments are dramatic, but by no means limited to unplanned (meta) comparisons between literature values. In the spirit of eliminating systematic errors, we see a major role for models as integrating as disparate a set of measures as possible. The dynamic and spatial biomodels of yore thought doomed by some by lack of data, will soon promote fresh study in the glaring light of overdetermination, i.e. more datapoints than adjustable parameters and feedback to the experiments justification for even more data for even more accuracy.We illustrate the above themes in the context of stress responses in wildtype and mutant human erythrocytes, E.coli and yeast time series. We assess measures of up to 19 metabolites, 400 proteins, and over 7000 RNAs. These measures touch most of the critical 34 metabolites in erythrocytes but only a tiny fraction of the over 1200 in E.coli. They so far quantitate fewer than 10 often have unknown covalent structure). For the RNAs (assayed with a dense set of oligon ucleotides) we see a rich, probably comprehensive set, including many unpredicted transcripts. So what are the next steps? Spatial effects seen for DNA-motifs at a few bp, hundreds, and thousands of bp (for three separate reasons) can be found by automatable methods. Time-series of molecular concentration data can be aligned by discrete and/or interpolative dynamic programming. Components of regulatory networks evident in time-series can be assessed by these independent models. The components of decay as well as steady-state levels have been modeled for a complete RNA sets. These time series benefit from the sharp specific transitions that can be achieved through conditional mutants and drugs (chemical biology in general). Overarching questions remain as to how we will systematize (automate) kinetic modeling and applications to a point analogous with strucural data modeling all the while connecting with issues of global quality of life?},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {103},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369178,
author = {Cs\H{o}r\H{o}s, Mikl\'{o}s},
title = {Fast recovery of evolutionary trees with thousands of nodes},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369178},
doi = {10.1145/369133.369178},
abstract = {We present a novel distance-based algorithm for evolutionary tree reconstruction. Our algorithm reconstructs the topology of a tree with n leaves in O(n2) time using O(n) working space. In the general Markov model of evolution the algorithm recovers the topology successfully with (1 — Ο(1)) probability from sequences with polynomial length in n. Moreover, for almost all trees, our algorithm achieves the same success probability on polylogarithmic sample sizes. The theoretical results are supported by simulation experiments involving trees with 500, 1895, and 3135 leaves. The topologies of the trees are recovered with high success from 2000 bp DNA sequences.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {104–113},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369181,
author = {Efrat, Alon and Hoffman, Frank and Kriegel, Klaus and Schultz, Christof and Wenk, Carola},
title = {Geometric algorithms for the analysis of 2D-electrophoresis gels},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369181},
doi = {10.1145/369133.369181},
abstract = {In proteomics 2-dimensional gel electrophoresis (2-DE) is a separation technique for proteins. The resulting protein spots can be identified by either using picking robots and subsequent mass spectrometry or by visual cross inspection of a new gel image with an already analyzed master gel. Difficulties especially arise from inherent noise and irregular geometric distortions in 2-DE images. Aiming at the automated analysis of large series of 2-DE images, or at the even more difficult interlaboratory gel comparisons, the bottleneck is to solve the two most basic algorithmic problems with high quality: Identifying protein spots and computing a matching between two images. For the development of the analysis software CAROL at Freie Universit\"{a}t Berlin we have reconsidered these two problems and obtained new solutions which rely on methods from computational geometry. Their novelties are: 1. Spot detection is also possible for complex regions formed by several “merged” (usually saturated) spots; 2. User-defined landmarks are not necessary for the matching. Furthermore, images for comparison are allowed to represent different parts of the entire protein pattern, which only partially “overlap”. The implementation is done in a client server architecture to allow queries via the Internet. We also discuss and point at related theoretical questions in computational geometry.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {114–123},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369183,
author = {Filkov, Vladmir and Skiena, Steven and Zhi, Jizu},
title = {Analysis techniques for microarray time-series data},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369183},
doi = {10.1145/369133.369183},
abstract = {We introduce new methods for the analysis of short-term time-series data, and apply them to gene expression data in yeast. These include (1) methods for automated period detection in a predominately cycling data set and (2) phase detection between phase-shifted cyclic data sets. We show how to properly correct for the problem of comparing correlation coefficents between pairs of sequences of different lengths and small alphabets. In particular, we show that the correlation coefficient of sequences over alphabets of size two can exhibit very counter-intuitive behavior when compared with the Hamming distance. Finally, we address the predictability of known regulators via time-series analysis, and show that less than 20\% of known regulatory pairs exhibit strong correlations in the Cho/Spellman data sets. By analyzing known regulatory relationships, we designed an edge detection function which identified candidate regulations with greater fidelity than standard correlation methods.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {124–131},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369182,
author = {Friedman, Nir and Ninio, Matan and Pe'er, Itsik and Pupko, Tal},
title = {A structural EM algorithm for phylogenetic inference},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369182},
doi = {10.1145/369133.369182},
abstract = {A central task in the study of evolution is the reconstruction of a phylogenetic tree from sequences of current-day taxa. A well supported approach to tree reconstruction performs maximum likelihood (ML) analysis. Unfortunately, searching for the maximum likelihood phylogenetic tree is computationally expensive. In this paper, we describe a new algorithm that uses Structural-EM for learning maximum likelihood trees. This algorithm is similar to the standard EM method for estimating branch lengths, except that during iterations of this algorithms the topology is improved as well as the branch length. The algorithm performs iterations of two steps. In the E-Step, we use the current tree topology and branch lengths to compute expected sufficient statistics, which summarize the data. In the M-Step, we search for a topology that maximizes the likelihood with respect to these expected sufficient statistics. As we show, searching for better topologies inside the M-step can be done efficiently, as opposed to standard search over topologies. We prove that each iteration of this procedure increases the likelihood of the topology, and thus the procedure must converge. We evaluate our new algorithm on both synthetic and real sequence data, and show that it is both dramatically faster and finds more plausible trees than standard search for maximum likelihood phylogenies.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {132–140},
numpages = {9},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369187,
author = {Frieze, Alan M. and Halld\'{o}rsson, Bjarni V.},
title = {Optimal sequencing by hybridization in rounds},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369187},
doi = {10.1145/369133.369187},
abstract = {Sequencing by hybridization (SBH) is a method for reconstructing a sequence over a small finite alphabet from a collection of probes (substrings). Substring queries can be arranged on an array (SBH chip) and then a combinatorial method is used to construct the sequence from its collection of probes. Technological constraints limit the number of substring queries that can be placed on a single SBH chip. We develop an idea of Margaritis and Skiena and propose an algorithm that uses a series of small SBH chips to sequence long strings while the number of probes used matches the information theoretical lower bound up to a constant factor.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {141–148},
numpages = {8},
keywords = {DNA sequencing, probabilistic analysis, sequencing by hybridization},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369188,
author = {Hallett, M. T. and Lagergren, J.},
title = {Efficient algorithms for lateral gene transfer problems},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369188},
doi = {10.1145/369133.369188},
abstract = {This paper develops a model for lateral gene transfer events (a.k.a. horizontal gene transfer events) between a set of gene trees T1, T2, …, Tk and a species tree S. To the best of our knowledge, this model possesses a higher degree of biological and mathematical soundness than any other model proposed in the literature. Among other biological considerations, the model respects the partial order of evolution implied by S. Within our model, we identify an activity parameter that measures the number of genes that are allowed to be simultaneously active in the genome of a taxa and show that finding the most parsimonious scenario that reconciles the disagreeing gene trees with the species tree is doable in polynomial time when the activity level and number of transfers are small, but intractable in general. To the best of our knowledge, all other models proposed in the literature assume implicitly that the activity is one. Finally, using a dataset of bacterial gene sequences from [4], our implementations found 5 optimal scenarios; one of which is the scenario proposed by the authors in [4].},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {149–156},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369190,
author = {Huson, Daniel H. and Reinert, Knut and Myers, Eugene},
title = {The greedy path-merging algorithm for sequence assembly},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369190},
doi = {10.1145/369133.369190},
abstract = {Two different approaches to determining the human genome are currently being pursued: one is the “clone-by-clone” approach, employed by the publicly-funded. Human Genome Project, and the other is the “whole genome shotgun” approach, favored by researchers at Celera Genomics. An interim strategy employed at Celera, called hierarchical assembly, makes use of preliminary data produced by both approaches. This paper introduces the Bactig Ordering Problem, which is a key problem that arises in this context, and presents an efficient heuristic called the greedy path-merginq algorithm that performs well on real data.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {157–163},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369191,
author = {Langmead, Christopher James and Donald, Bruce Randall},
title = {Extracting structural information using time-frequency analysis of protein NMR data},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369191},
doi = {10.1145/369133.369191},
abstract = {High-throughput, data-directed computational protocols for Structural Genomics (or Proteomics) are required in order to evaluate the protein products of genes for structure and function at rates comparable to current gene-sequencing technology. To develop such methods, new algorithms are required that can quickly extract significantly more structural information from sparse experimental data. This paper presents a new class of signal processing algorithms for nuclear magnetic resonance (NMR) structural biology, based on time-frequency analysis of chemical shift dynamics.A novel approach to multidimensional NMR analysis is proposed in which the data are interpreted in the time-frequency domain, as opposed to the traditional frequency domain. Time-frequency analysis (TFA) exposes behavior orthogonal to the magnetic coherence transfer pathways, thus affording new avenues of NMR discovery. An implementation yielding new biophysical results is discussed. In particular, we demonstrate the heretofore unknown presence of through-space inter-atomic distance information within 15N-edited heteronuclear single-quantum coherence(15N HSQC) data. A biophysical model explains these results, and is supported by further experiments on simulated spectra.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {164–175},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369192,
author = {Kececioglu, John and Ju, Jun},
title = {Separating repeats in DNA sequence assembly},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369192},
doi = {10.1145/369133.369192},
abstract = {One of the key open problems in large-scale DNA sequence assembly is the correct reconstruction of sequences that contain repeats. A long repeat can confound a sequence assembler into falsely overlaying fragments that sample its copies, effectively compressing out the repeat in the reconstructed sequence. We call the task of correcting this compression by separating the overlaid fragments into the distinct copies they sample, the repeat separation problem. We present a rigorous formulation of repeat separation in the general setting without prior knowledge of consensus sequences of repeats or their number of copies. Our formulation decomposes the task into a series of four subproblems, and we design probabilistic tests or combinatorial algorithms that solve each subproblem. The core subproblem separates repeats using the so-called k-median problem in combinatorial optimization, which we solve using integer linear-programming. Experiments with an implementation show we can separate fragments that are overlaid at 10 times the coverage with very few mistakes in a few seconds of computation, even when the sequencing error rate and the error rate between copies are identical. To our knowledge this is the first rigorous and fully general approach to separating repeats that directly addresses the problem.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {176–183},
numpages = {8},
keywords = {k-median problem, computational biology, disambiguating repeats, shotgun sequencing},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369194,
author = {Kohlbacher, O. and Burchardt, A. and Moll, A. and Hildebrandt, A. and Bayer, P. and Lenhof, H.-P},
title = {A NMR-spectra-based scoring function for protein docking},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369194},
doi = {10.1145/369133.369194},
abstract = {A well studied problem in the area of Computational Molecular Biology is the so-called Protein-Protein Docking problem (PPD) that can be formulated as follows: Given two proteins A and B that form a protein complex, compute the 3D-structure of the protein complex AB. Protein docking algorithms can be used to study the driving forces and reaction mechanisms of docking processes. They are also able to speed up the lenghty process of experimental structure elucidation of protein complexes by proposing potential structures. In this paper, we are discussing a variant of the PPD-problem where the input consists of the tertiary structures of A and B plus an unassigned 1H-NMR spectrum of the complex AB. We present a new scoring function for evaluating and ranking potential complex structures produced by a docking algorithm. The scoring function computes a “theoretical” 1H-NMR spectrum for each tentative complex structure and subtracts the calculated spectrum from the experimental spectrum. The absolute areas of the difference spectra are then used to rank the potential complex structures. In contrast to formerly published approaches (e.g. Morelli et. al. [38]) we do not use distance constraints (intermolecular NOE constraints). We have tested the approach with the bound conformations of four protein complexes whose three-dimensional structures are stored in the PDB data bank [5] and whose 1H-NMR shift assignments are available from the BMRB database (BioMagResBank [47]).In all examples, the new scoring function produced very good rankings of the structures. The best result was obtained for an example, where all standard scoring functions failed completely. Here, our new scoring function achieved an almost perfect separation between good approximations of the true complex structure and false positives.Unfortunately, the number of complexes with known structure and available spectra is very small. Nevertheless, these experiments indicate that scoring functions based on comparisons of one- or multi-dimensional NMR spectra might be a good instrument to improve the reliability and accuracy of docking predictions and perhaps also of protein structure predictions (threading).},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {184–192},
numpages = {9},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369199,
author = {Lancia, Giuseppe and Carr, Robert and Walenz, Brian and Istrail, Sorin},
title = {101 optimal PDB structure alignments: a branch-and-cut algorithm for the maximum contact map overlap problem},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369199},
doi = {10.1145/369133.369199},
abstract = {Structure comparison is a fundamental problem for structural genomics. A variety of structure comparison methods were proposed and several protein structure classification servers e.g., SCOP, DALI, CATH, were designed based on them, and are extensively used in practice. This area of research continues to be very active, being energized bi-annually by the CASP folding competitions, but despite the extraordinary international research effort devoted to it, progress is slow. A fundamental dimension of this bottleneck is the absence of rigorous algorithmic methods. A recent excellent survey on structure comparison by Taylor et.al. [23] records the state of the art of the area: In structure comparison, we do not even have an algorithm that guarantees an optimal answer for pairs of structures …In this paper we provide the first rigorous algorithm for structure comparison. Our method is based on developing an effective integer linear programming (IP) formulation of protein structure contact maps overlap (CMO), and a branch-and-cut strategy that employs lower-bounding heuristics at the branch nodes. Our algorithms identified a gallery of optimal and near-optimal structure alignments for pairs of proteins from the Protein Data Bank with up to 80 amino acids and about 150 contacts each — problems of instance size of about 300. Although these sizes also reflect our current limitations, these are the first provable optimal and near-optimal algorithms in the literature for a measure of structure similarity which sees extensive practical use. At the heart of our success in finding optimal alignments is a reduction of the CMO optimization to the maximum independent set (MIS) problem on special graphs. For CMO instances of size 300, the corresponding MIS graph instance contains about 10,000 nodes. While our algorithms are able to solve to optimality MIS problem of these sizes, the known optimal algorithms for the MIS on general graphs can at present only solve instances with up to a few hundred nodes. This is the first effective use of IP methods in protein structure comparison; the biomolecular structure literature contains only one other effective IP method devoted to RNA comparison, due to Lenhof et.al. [18].The hybrid heuristic approach that worked well for providing lower bounds in the branch and cut algorithm was tried on large proteins in a test set suggested by Jeffrey Skolnick. It involved 33 proteins classified into four families: Flavodoxin-like fold CheY-related, Plastocyanin, TIM Barrel, and Ferratin. Out of the set of all 528 pairwise structure alignments, we have validated the clustering with a 98.7\% accuracy (1.3\% false negatives and 0\% false positives).},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {193–202},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369201,
author = {Lang, Franz},
title = {Comparative analysis of organelle genomes, a biologist's view of computational challenges (abstract only)},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369201},
doi = {10.1145/369133.369201},
abstract = {With genomic data (generated by classical, functional, structural, proteo- and other `omic' approaches) accumulating at a stupendous rate, there is an ever increasing need for the development of new, more efficient and more sensitive computational methods. To highlight aspects of our computational needs, we will present results that emerged from the comparative genome analysis of mitochondria. Having originated from an alpha-proteobacterial endosymbiont, these eukaryotic organelles contain small and extremely variable genomes, and are thus perfect model systems for the much more complex eubacterial and archaeal genomes. We are currently in vestigating mitochondrial DNAs (mtDNAs) in a lineage of unicellular, primitive protistan eukaryotes, the jakobids, with the aim to understand the evolution of mitochondrial genomes, genes and their regulation. Because these organisms are difficult to grow, biochemical approaches aimed at understanding gene regulation are laborious, thus it is possible to capitalize considerably from predictions on genome and gene organization, and regulatory elements. Contrary to approaches in which molecular data (gene order, sequence similarities) are used to infer the phylogenetic relationships among a group of organism, we know their phylogeny and employ this information to identify and model more or less conserved genetic elements and structural RNA genes that are difficult to spot by conventional methods, in a phylogenetic-comparative approach.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {203},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369202,
author = {Li, Wentian},
title = {DNA segmentation as a model selection process},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369202},
doi = {10.1145/369133.369202},
abstract = {Previous divide-and-conquer segmentation analyses of DNA sequences do not provide a satisfactory stopping criterion for the recursion. This paper proposes that segmentation be considered as a model selection process. Using the tools in model selection, a limit for the stopping criterion on the relaxed end can be determined. The Bayesian information criterion, in particular, provides a much more stringent stopping criterion than what is currently used. Such a stringent criterion can be used to delineate larger DNA domains. A relationship between the stopping criterion and the average domain size is empirically determined, which may aid in the determination of isochore borders.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {204–210},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369214,
author = {Lin, Guo-Hui and Ma, Bin and Zhang, Kaizhong},
title = {Edit distance between two RNA structures},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369214},
doi = {10.1145/369133.369214},
abstract = {Arc-annotated sequences are useful in representiug the structural information of RNA sequences. Typically, RNA secondary and tertiary structures could be represented by a set of nested arcs and a set of crossing arcs, respectively. As the specified RNA functions are determined by the specified molecular confirmation and therefore the specified secondary and tertiary structures, the comparison between RNA secondary and tertiary structures have received much attention recently. In this paper, we propose the notion of edit distance to measure the similarity between two RNA secondary and tertiary structures, by incorporating the various edit operations performing on both bases and arcs (base-pairs). Several algorithms are presented to compute the edit distance two RNA sequences with various arc structures and under various score schemes, either exactly or approximately. Preliminary experimental tests confirm that our definition of edit distance and the computation model are among the most reasonable ones ever studied in the literature.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {211–220},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369216,
author = {Lindpaintner, Klaus},
title = {Genetics and genemoics: impact on drug discovery and development},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369216},
doi = {10.1145/369133.369216},
abstract = {complex diseases is limited by a still rudimentary understanding of the molecular basis of disease as well as of drug action. At the heart of this is our current inability to account for inter-individual differences in disease etiology and drug response. These inter-individual differences are determined, to a large extent, by inherited predispositions and susceptibilities. Knowledge of the genetic differences that explain these individual characteristics, and based upon it, the development of specific diagnostics and therapeutics, will therefore be critical for the successful transition to a future progress in health care.The impact of genetics and genomics will leave its mark along all steps involved in the creation of a new medicine: in the discovery of new targets that carry-inherently, because genetic linkage implies causation-a greater likelihood of success;  in the discovery phase of a new drug aimed at an existing target, where the knowledge of molecular variation of this target (SNPs) may provide clues to achieve higher selectivity; where genetic epidemiology studies will provide added value by validating the target; and where large scale gene expression profiling (gene chips) will help select compounds with a higher likelihood for ultimate success at an early stage;  and in the development phase of an drug undergoing clinical evaluation, where pharmacogenetic studies, and genotype-specific patient selection may allow recognition and definition of drug-responders and non-responders, or help decrease the likelihood of adverse events.  Although the impact of genetic and genomic investigation will certainly accelerate progress in biomedical research, we believe it will do so in an evolutionary fashion, and as a logical extension of the history of medical progress towards a more detailed understanding of disease and the resultant more refined differential diagnosis as well as more accurate prospective risk assessment. If any, the fundamental change we are going to witness in the years to come is a (paradigmatic) shift from today's largely clinical disease definition and diagnosis to a molecular definition and diagnosis of disease. This shift is likely to greatly increase the importance of in-vitro diagnostics and will mandate, much more than is the case today, an integrated approach of diagnostics and therapeutics. Ultimately, we expert to derive the benefit of more successful, and more cost-effective medicines, and of possibly being able to prevent (or delay), rather than treat disease.It is important to realize that genetic research and testing are areas of great public concern, and that a more comprehensive dialogue between scientists and the public is urgently needed to address the societal, ethical, legal issues that are being raised. Only then will we be able to truly take advantage of the significant advances in medical knowledge that genetic research will make possible, and fully realize the potential of these approaches towards the ultimate goal of all our striving, improving the human condition.The utility of most drugs prescribed today for common, complex diseases is limited by a still rudimentary understanding of the molecular basis of disease as well as of drug action. At the heart of his is our current inability to account for inter-individual differences in disease etiology and drug response. These inter-individual differences are determined, to a large extent, by inherited predispositions and susceptibilities. Knowledge of the genetic differences that explain these individual characteristics, and based upon it, the development of specific diagnostics and therapeutics, will therefore be critical for the successful transition to a future progress in health care.The impact of genetics and genomics will leave its mark along all steps involved in the creation of a new medicine: in the discovery of new targets that carry-inherently, because genetic linkage implies causation-a greater likelihood of success;  in the discovery of a new drug aimed at an existing target, where the knowledge of molecular variation of this target (SNPs) may provide clues to achieve higher selectivity; where genetic apidemiology studies will provide added value by validating the target; and where large scale gene expression profiling (gene chips) will help select compounds with a higher likelihood for ultimate success at an early stage;  and in the development phase of an drug undergoing clinical evaluation, where pharmacogenetic studies, and genotype-specific patient selection may allow recognition and definition of drug-responders and non-responders, or help decrease the likelihood of adverse events.  Although the impact of genetic and genomic investigation will certainly accelerate progress in biomedical research, we believe it will do so in an evolutionary fashion, and as a logical extension of the history of medical progress towards a more detailed understanding of disease and the resultant more refined differential diagnosis as well as more accurate prospective risk assessment. If any, the fundamental change we are going to witness in the years to come is a (paradigmatic) shift from today's largely clinical disease definition and diagnosis to a molecular definition and diagnosis of disease. This shift is likely to greatly increase the importance of in-vitro diagnostics and will mandate, much more than is the case today, an integrated approach of diagnostics and therapeutics. Ultimately, we expect to derive the benefit of more successful, and more cost-effective medicines, and of possibly being able to prevent (or delay), rather than treat disease.It is important to realize that genetic research and testing are areas of great public concern, and that a more comprehensive dialogue between scientists and the public is urgently needed to address the societal, ethical, legal issues that are being raised. Only then will we be able to truly take advantage of the significant advances in medical knowledge that genetic research will make possible, and fully realize the potential of these approaches towards the ultimate goal of all our striving, improving the human condition.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {221–222},
numpages = {2},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369217,
author = {Martin, Yvonne},
title = {The role of computational chemistry in translating genomic information into bioactive small molecules (abstract only)},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369217},
doi = {10.1145/369133.369217},
abstract = {Although genomic information provides many potential targets for drug discovery, the challenge is to convert this information into drugs that cure human disease. Workers in computer assisted molecular design and chemometrics have developed a number of techniques to aid this process. Examples include selecting diverse compounds for high throughput screening, designing universal or targeted combinatorial libraries, and using a variety of computational techniques to forecast binding affinitiy and bioavailability of compounds. This lecture will summarize recent advances in these areas.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {223},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369218,
author = {Myers, Gene},
title = {Comparing sequence scaffolds},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369218},
doi = {10.1145/369133.369218},
abstract = {The DNA sequence assembler we built for the whole genome shotgun assembly of the human genome, utilizes end-reads of inserts to order and orient assembled contigs into scaffolds for which the distances between consecutive contigs are statistically characterized. We consider the problem of comparing two such scaffolds. Applications include comparison of two distinct assemblies for mutual confirmation, and comparison of scaffold assemblies of BACs to determine a whole genome tiling of the BACs. We formalize the problem and develop efficient algorithms for a number of variations of the problem, the essential result being a sparse algorithm that refines gap estimates based on the overlap evidence.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {224–230},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369220,
author = {Navarro, Gonzalo and Raffinot, Mathieu},
title = {Fast and simple character classes and bounded gaps pattern matching, with application to protein searching},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369220},
doi = {10.1145/369133.369220},
abstract = {The problem of fast searching of a pattern that contains Classes of characters and Bounded size Gaps (CBG) in a text has a wide range of applications, among which a very important one is protein pattern matching (for instance, one PROSITE protein site is associated with the CBG [RK] — x(2, 3) — [DE] — x(2, 3) — Y, where the brackets match any of the letters inside, and x(2, 3) a gap of length between 2 and 3). Currently, the only way to search a CBG in a text is to convert it into a full regular expression (RE). However, a RE is more sophisticated than a CBG, and searching it with a RE pattern matching algorithm complicates the search and makes it slow. This is the reason why we design in this article two new practical CBG matching algorithms that are much simpler and faster than all the RE search techniques. The first one looks exactly once at each text character. The second one does not need to consider all the text characters and hence it is usually faster than the first one, but in bad cases may have to read the same text character more than once. We then propose a criterion based on the form of the CBG to choose a-priori the fastest between both. We performed many practical experiments using the PROSITE database, and all them show that our algorithms are the fastest in virtually all cases.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {231–240},
numpages = {10},
keywords = {PROSITE, bit-parallelism, computational biology, information retrieval, pattern matching},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369227,
author = {Pachter, Lior and Alexandersson, Marina and Cawley, Simon},
title = {Applications of generalized pair hidden Markov models to alignment and gene finding problems},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369227},
doi = {10.1145/369133.369227},
abstract = {Hidden Markov models (HMMs) have been successfully applied to a variety of problems in molecular biology, ranging from alignment problems to gene finding and annotation. Alignment problems can be solved with pair HMMs, while gene finding programs rely on generalized HMMs in order to model exon lengths. In this paper we introduce the generalized pair HMM (GPHMM), which is an extension of both pair and generalized HMMs. We show how GPHMMs, in conjunction with approximate alignments, can be used for cross-species gene finding, and describe applications to DNA-cDNA and DNA-protein alignment. GPHMMs provide a unifying and probabilistically sound theory for modeling these problems.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {241–248},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369228,
author = {Pavlidis, Paul and Weston, Jason and Cai, Jinsong and Grundy, William Noble},
title = {Gene functional classification from heterogeneous data},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369228},
doi = {10.1145/369133.369228},
abstract = {In our attempts to understand cellular function at the molecular level, we must be able to synthesize information from disparate types of genomic data. We consider the problem of inferring gene functional classifications from a heterogeneous data set consisting of DNA microarray expression measurements and phylogenetic profiles from whole-genome sequence comparisons. We demonstrate the application of the support vector machine (SVM) learning algorithm to this functional inference task. Our results suggest the importance of exploiting prior information about the heterogeneity of the data. In particular, we propose an SVM kernel function that is explicitly heterogeneous. We also show how to use knowledge about heterogeneity to aid in feature selection.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {249–255},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369230,
author = {Pevzner, Pavel A. and Tang, Haixu and Waterman, Michael S.},
title = {A new approach to fragment assembly in DNA sequencing},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369230},
doi = {10.1145/369133.369230},
abstract = {For the last twenty years fragment assembly in DNA sequencing followed the “overlap - layout - consensus” paradigm that is used in all currently available assembly tools. Although this approach proved to be useful in assembling clones, it faces difficulties in genomic shotgun assembly: the existing algorithms make assembly errors and are often unable to resolve repeats even in prokaryotic genomes. Biologists are well-aware of these errors and are forced to carry additional experiments to verify the assembled contigs.We abandon the classical “overlap - layout - consensus” approach in favor of a new Eulerian Superpath approach that, for the first time, resolves the problem of repeats in fragment assembly. Our main result is the reduction of the fragment assembly to a variation of the classical Eulerian path problem. This reduction opens new possibilities for repeat resolution and allows one to generate error-free solutions of the large-scale fragment assembly problems. The major improvement of EULER over other algorithms is that it resolves all repeats except long perfect repeats that are theoretically impossible to resolve without additional experiments.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {256–267},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369231,
author = {Ptashne, Mark},
title = {Imposing specificity by regulated localization (abstract only)},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369231},
doi = {10.1145/369133.369231},
abstract = {Many biologically important enzymes — RNA polymerases, RNA splicing enzymes, ubiquinating enzymes, certain kinases and proteases — are “`regulated”' by being brought to one or another of many potential substrates by auxiliary docking proteins (e.g. transcriptional activators). These regulatory interactions require interactions between simple adhesive (but specific) surfaces. This kind of regulation is highly `evolvable' : new and expanded meanings to signals are readily generated by simple changes in protein surfaces.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {268},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369233,
author = {Shamir, Ron and Tsur, Dekel},
title = {Large scale sequencing by hybridization},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369233},
doi = {10.1145/369133.369233},
abstract = {Sequencing by hybridization is a method for reconstructing a DNA sequence based on its k-mer content. This content, called the spectrum of the sequence, can be obtained from hybridization with a universal DNA chip. However, even with a sequencing chip containing all 49 9-mers and assuming no hybridization errors, only about 400 bases-long sequences can be reconstructed unambiguously.Drmanac et al. suggested sequencing long DNA targets by obtaining spectra of many short overlapping fragments of the target, inferring their relative positions along the target and computing spectra of subfragments that are short enough to be uniquely recoverable. Drmanac et al. do not treat the realistic case of errors in the hybridization process. In this paper we study the effect of such errors. We show that the probability of ambiguous reconstruction in the presence of (false negative) errors is close to the probability in the errorless case. More precisely, the ratio between these probabilities is 1 + Ο(p/(1 - p)4 · 1/d) where d is the average distance between neighboring subfragments, and p is the probability of a false negative.We also obtain lower and upper bounds for the probability of unambiguous reconstruction based on errorless spectrum. For realistic chip sizes, these bounds are tighter than those given by Arratia et al. Finally, we report results on simulations with real DNA sequences, showing that even in the presence of 50\% false negative errors, a target of cosmid length can be recovered with less than 0.1\% miscalled bases.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {269–277},
numpages = {9},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369235,
author = {Sharp, Philip},
title = {RNA biology and the genome (abstract only)},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369235},
doi = {10.1145/369133.369235},
abstract = {The sequence of vertebrate genome is expressed by RNA splicing producing mRNA. Interpreting the genome requires understanding the sequences recognized by the nuclear factors and spliceosomia executing removal of introns. The status of this area of science will be reviewed. Conversely, genome sequences can be searched for sequences which specify RNA splicing and are common of many genes. The availability of genome sequences from a variety of species will make the latter approach much more powerful. The recently discovered RNA interference (RNAi) process is evolutionarily old and related processes are important for control of expression of repetitive genes in some organisms. RNAi can be initiated by conversion of double strand RNA into 21-23 nucleotide RNAs which can base pair with mRNAs and direct their cleavage. These short RNAs may also direct silencing of genes by other mechanisms. Over half of the genome of many organisms is composed of repetitive sequences and it is possible that RNAi may silence these sequences.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {278},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369238,
author = {Singh, Mona and Kim, Peter S.},
title = {Towards predicting coiled-coil protein interactions},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369238},
doi = {10.1145/369133.369238},
abstract = {Protein-protein interactions play a central role in many cellular functions, and as whole-genome data accumulates, computational methods for predicting these interactions become increasingly important. Computational methods have already proven to be a useful first step for rapid genome-wide identification of putative protein structure and function, but research on the problem of computationally determining biologically relevant partners for given protein sequences is just beginning. In this paper, we approach the problem of predicting protein-protein interactions by focusing on the 2- stranded coiled-coil motif. We introduce a computational method for predicting coiled-coil protein interactions, and give a novel framework that is able to use both genomic sequence data and experimental data in making these predictions. Cross-validation tests show that the method is able to predict many aspects of protein-protein interactions mediated by the coiled-coil motif, and suggest that this methodology can be used as the basis for genome-wide prediction of coiled-coil protein interactions.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {279–286},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369239,
author = {Song, Guang and Amato, Nancy M.},
title = {Using motion planning to study protein folding pathways},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369239},
doi = {10.1145/369133.369239},
abstract = {We present a framework for studying protein folding pathways and potential landscapes which is based on techniques recently developed in the robotics motion planning community. In particular, our work uses Probabilistic Roadmap (PRM) motion planning techniques which have proven to be very successful for problems involving high-dimensional configuration spaces. Our results applying PRM techniques to several small proteins (60 residues) are very encouraging. The framework enables one to easily and efficiently compute folding pathways from any denatured starting state to the native fold. This aspect makes our approach ideal for studying global properties of the protein's potential landscape. For example, our results show that folding pathways from different starting denatured states sometimes share some common `gullies', mainly when they are close to the native fold. Such global issues are difficult to simulate and study with other methods.Our focus in this work is to study the protein folding mechanism assuming we know the native fold. Therefore, instead of performing fold prediction, we aim to study issues related to the folding process, such as the formation of secondary and tertiary structure, and the dependence on the initial conformation. Our results indicate that for some proteins, secondary structure clearly forms first while for others the tertiary structure is obtained more directly, and moreover, these situations seem to be differentiated in the distributions of the conformations sampled by our technique. We also find that the formation order is independent of the starting denatured conformation. We validate our results by comparing the secondary structure formation order on our paths to known pulse-labeling experimental results. This indicates the promise of our approach for studying proteins for which experimental results are not available.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {287–296},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369241,
author = {Tang, Mengxiang and Waterman, Michael and Yooseph, Shibu},
title = {Zinc finger gene clusters and tandem gene duplication},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369241},
doi = {10.1145/369133.369241},
abstract = {Zinc finger genes in mammalian genomes are frequently found to occur in clusters with cluster members appearing in a tandem array on the chromosome. It has been suggested that in situ gene duplication events are primarily responsible for the evolution of such clusters. The problem of inferring the series of duplication events responsible for producing clustered families is different from the standard phylogeny problem. In this paper we study this inference problem using a graph called Duplication Model that captures the series of duplication events while taking into account the observed order of the genes on the chromosome. We provide algorithms to reconstruct a duplication model for a given data set. We use our method to hypothesise the series of duplication events that may have produced the ZNF45 family that appears on human chromosome 19.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {297–304},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369253,
author = {Thijs, Gert and Marchal, Kathleen and Lescot, Magali and Rombauts, Stephane and De Moor, Bart and Rouz\'{e}, Pierre and Moreau, Yves},
title = {A Gibbs sampling method to detect over-represented motifs in the upstream regions of co-expressed genes},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369253},
doi = {10.1145/369133.369253},
abstract = {Microarray experiments can reveal useful information on the transcriptional regulation. We try to find regulatory elements in the region upstream of translation start of coexpressed genes. Here we present a modification to the original Gibbs Sampling algorithm [12]. We introduce a probability distribution to estimate the number of copies of the motif in a sequence. The second modification is the incorporation of a higher-order background model. We have successfully tested our algorithm on several data sets. First we show results on two selected data set: sequences from plants containing the G-box motif and the upstream sequences from bacterial genes regulated by O2-responsive protein FNR. In both cases the motif sampler is able to find the expected motifs. Finally, the sampler is tested on 4 clusters of coexpressed genes from a wounding experiment in Arabidopsis thaliana. We find several putative motifs that are related to the pathways involved in the plant defense mechanism.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {305–312},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.374565,
author = {Wilm, Matthias},
title = {Creating the backbone for the virtual cell (abstract only): cell mapping projects on the run},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.374565},
doi = {10.1145/369133.374565},
abstract = {The ability to identify proteins with mass spectrometry has a profound impact on biological research. Its long term effects can only be compared with the significance PCR enzyme had for the advancement of biological science.Suddenly proteins can be identified having only minute quantities available. The efforts invested in to the technology lead to a constant increase in throughput - comparable to the development of DNA sequencing technology.Biological mechanisms can be studied directly on the protein level. There are two research directions: protein expression based proteomics and cell mapping or functional proteomics.Protein expression based proteomics is focussing on the understanding of cellular states by analyzing the protein expression level of as many proteins as possible. In cell mapping experiments the protein - protein interaction network within a cell is analyzed.At the EMBL a protein complex purification technique, called tandem affinity purification (TAP), was developed which allows to rapidly purify very specifically non covalently interacting proteins which can be subsequently analyzed with mass spectrometry (cell mapping approach). The complexes assemble in vivo, are purified as such and characterized. The technique had been developed in yeast and efforts are under way to establish similar methods in higher eukaryotes. The comparison of this technique with similar experiments based on two hybrid screens demonstrate that the protein based approach is more specific and functional data suggest that it is more complete. This may be due to the fact that the complexes assemble first in their native environment and are then pulled out for characterization.The organization of all the proteins into groups of physically in teracting proteins is an important step towards understanding their functional role within a cell. To a large degree cellular processes organize themselves based on the non-covalent affinity of proteins. Protein based approaches as exemplified by the tandem affinity purification method in conjunction with mass spectrometry are very important in analyzing this functional infra structure. The identification of all the proteins involved in a particular biological mechanism and the characterization of their interaction constants may provide the physical data to simulate the process by a computer.},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
pages = {313},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

@inproceedings{10.1145/369133.369333,
title = {Creating the backbone for the virtual cell: cell mapping projects on the run},
year = {2001},
isbn = {1581133537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/369133.369333},
doi = {10.1145/369133.369333},
booktitle = {Proceedings of the Fifth Annual International Conference on Computational Biology},
articleno = {1},
location = {Montreal, Quebec, Canada},
series = {RECOMB '01}
}

