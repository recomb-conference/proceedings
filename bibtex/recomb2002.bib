@inproceedings{10.1145/565196.565197,
author = {Abagyan, Ruben},
title = {Computational structural proteomics and virtual ligand screening},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565197},
doi = {10.1145/565196.565197},
abstract = {Large scale discovery of new human genes, gene families and isozymes creates an exciting biomedical opportunity. Advanced modeling by homology techniques can be employed to generate 3D models for most of the interesting new gene family members. This opens many new opportunities for structural structure based functional annotation and molecular design. Several key computational technologies enable structural protein modeling and rational drug design: sequence-structure relationships, multiple sequence alignments, threading molecular mechanics; global energy optimization; protein modeling by homology; structure based protein functional annotation (e.g. prediction of possible active sites, small molecule binding sites, protein-protein interaction sites, flexibility annotation); protein design and redesign using structure prediction algorithms; structural immunology, e.g. derivation and design of peptide sequence specific for a particular HLA presenting molecule based on the HLA structure; peptide-protein docking; protein-protein docking; small molecule docking and virtual ligand screening; cheminformatics algorithms and virtual chemistry.We have been developing the internal coordinate mechanics approach to structure prediction for the last 16 years. It is aimed at de novo prediction of large structural rearrangements. To achieve that goal an ICM program has been developed and combined accurate energy functions with powerful global optimization algorithm in the internal coordinate space.The most recent developments include on-the-fly calculations of accurate Poisson electrostatics using the efficient boundary element algorithm based on the analytical molecular surface. This allows to predict de novo a solution structure of a 23-residue peptide.New computational technology for virtual ligand screening allows to generate alternative receptor conformations and perform flexible docking of hundreds of thousands of virtual compounds to the binding site. The success of this technology is demonstrated on several benchmarks and in six experimental projects which tested the prediction results. For well defined binding pockets only about the initial compound set actually needs to be synthesized and tested. Using ICM docking and scoring technology we have identified new ligands in a number of cases, including cases in which a model was used instead of a crystallographic structure and a case in which protein-protein interaction has been targeted.The most interesting recent results tested experimentally include: the de novo design of novel antagonists of thyroid hormone receptor, for which no antagonists were known before and, consequently, no antagonist-bound structure was known; de novo design of ligands targeting Ephrin-Ephrin receptor interactions.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {1},
numpages = {1},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565198,
author = {Amato, Nancy M. and Dill, Ken A. and Song, Guang},
title = {Using motion planning to map protein folding landscapes and analyze folding kinetics of known native structures},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565198},
doi = {10.1145/565196.565198},
abstract = {We present a novel approach for studying the kinetics of protein folding. The framework has evolved from robotics motion planning techniques called probabilistic roadmap methods (prms) that have been applied in many diverse fields with great success. In our previous work, we used a Prm-based technique to study protein folding pathways of several small proteins and obtained encouraging results. In this paper, we describe how our motion planning framework can be used to study protein folding kinetics. In particular, we present a refined version of our Prm-based framework and describe how it can be used to produce potential energy landscapes, free energy landscapes, and many folding pathways all from a single roadmap which is computed in a few hours on a desktop PC. Results are presented for 14 proteins. Our ability to produce large sets of unrelated folding pathways may potentially provide crucial insight into some aspects of folding kinetics, such as proteins that exhibit both two-state and three-state kinetics, that are not captured by other theoretical techniques.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {2–11},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565199,
author = {Apaydin, Mehmet Serkan and Brutlag, Douglas L. and Guestrin, Carlos and Hsu, David and Latombe, Jean-Claude},
title = {Stochastic roadmap simulation: an efficient representation and algorithm for analyzing molecular motion},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565199},
doi = {10.1145/565196.565199},
abstract = {Classic techniques for simulating molecular motion, such as the Monte Carlo and molecular dynamics methods, generate individual motion pathways one at a time and spend most of their time trying to escape from the local minima of the energy landscape of a molecule. Their high computational cost prevents them from being used to analyze many pathways. We introduce Stochustic Roadmap Sirrrcllation (SRS), a new approach for exploring the kinetics of molecular motion by simultaneously examining multiple pathways encoded compactly in a graph, called a roadmap. A roadmap is computed by sampling a molecule's conformation space at random. The computation does not suffer from the localminima problem encountered with existing methods. Each path in the roadmap represents a potential motion pathway and is associated with a probability indicating the likelihood that the molecule follows this pathway. By viewing the roadmap as a Markov chain, we can efficiently compute kinetic properties of molecular motion over the entire molecular energy landscape. We also prove that, in the limit, SRS converges to the same distribution as Monte Carlo simulation. To test the effectiveness of our approach, we apply it to the computation of the transmission coefficients for protein folding, an important order parameter that measures the "kinetic distance" of a protein's conformation to its native state Our computational studies show that SRS obtains more accurate results and achieves several orders- of- magnitude reduction in computation time, compared with Monte Carlo simulatio.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {12–21},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565200,
author = {Apostolico, Alberto and Bock, Mary Ellen and Lonardi, Stefano},
title = {Monotony of surprise and large-scale quest for unusual words},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565200},
doi = {10.1145/565196.565200},
abstract = {The problem of characterizing and detecting recurrent sequence patterns such as substrings or motifs and related associations or rules is variously pursued in order to compress data, unveil structure, infer succinct descriptions, extract and classify features, etc. In Molecular Biology, exceptionally frequent or rare words in bio-sequences have been implicated in various facets of biological function and structure. The discovery, particularly on a massive scale, of such patterns poses interesting methodological and algorithmic problems, and often exposes scenarios in which tables and synopses grow faster and bigger than the raw sequences they are meant to encapsulate. In previous study, the ability to succinctly compute, store, and display unusual substrings has been linked to a subtle interplay between the combinatorics of the subwords of a word and local monotonicities of some scores used to measure the departure from expectation. In this paper, we carry out an extensive analysis of such monotonicities for a broader variety of scores. This supports the construction of data structures and algorithms capable of performing global detection of unusual substrings in time and space linear in the subject sequences, under various probabilistic models.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {22–31},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565201,
author = {Arndt, Peter F. and Burge, Christopher B. and Hwa, Terence},
title = {DNA sequence evolution with neighbor-dependent mutation},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565201},
doi = {10.1145/565196.565201},
abstract = {We introduce a model of DNA sequence evolution which can account for biases in mutation rates that depend on the identity of the neighboring bases. An analytic solution for this class of models is developed by adopting well-known methods of nonlinear dynamics. Results are presented for the CpG-methylation-deamination process which dominates point substitutions in vertebrates. The dinucleotide frequencies generated by the model (using empirically obtained mutation rates) match the overall pattern observed in non-coding DNA. A web-based tool has been constructed to compute single- and dinucleotide frequencies for arbitrary neighbor-dependent mutation rates. Also provided is the backward procedure to infer the mutation rates using maximum likelihood analysis given the observed single- and dinucleotide frequencies. Reasonable estimates of the mutation rates can be obtained very efficiently, using generic non-coding DNA sequences as input, after masking out long homonucleotide subsequences. Our method is much more convenient and versatile to use than the traditional method of deducing mutation rates by counting mutation events in carefully chosen sequences. More generally, our approach provides a more realistic but still tractable description of non-coding genomic DNA, and may be used as a null model for various sequence analysis applications.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {32–38},
numpages = {7},
keywords = {CpG-methylation-deamination, DNA sequence evolution, single-and dinucleotide frequencies},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565202,
author = {Bar-Joseph, Ziv and Gerber, Georg and Gifford, David K. and Jaakkola, Tommi S. and Simon, Itamar},
title = {A new approach to analyzing gene expression time series data},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565202},
doi = {10.1145/565196.565202},
abstract = {We present algorithms for time-series gene expression analysis that permit the principled estimation of unobserved time-points, clustering, and dataset alignment. Each expression profile is modeled as a cubic spline (piecewise polynomial) that is estimated from the observed data and every time point influences the overall smooth expression curve. We constrain the spline coefficients of genes in the same class to have similar expression patterns, while also allowing for gene specific parameters. We show that unobserved time-points can be reconstructed using our method with 10-15\% less error when compared to previous best methods. Our clustering algorithm operates directly on the continuous representations of gene expression profiles, and we demonstrate that this is particularly effective when applied to non-uniformly sampled data. Our continuous alignment algorithm also avoids difficulties encountered by discrete approaches. In particular, our method allows for control of the number of degrees of freedom of the warp through the specification of parameterized functions, which helps to avoid overfitting. We demonstrate that our algorithm produces stable low-error alignments on real expression data and further show a specific application to yeast knockout data that produces biologically meaningful results.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {39–48},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565203,
author = {Ben-Dor, Amir and Chor, Benny and Karp, Richard and Yakhini, Zohar},
title = {Discovering local structure in gene expression data: the order-preserving submatrix problem},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565203},
doi = {10.1145/565196.565203},
abstract = {This paper concerns the discovery of patterns in gene expression matrices, in which each element gives the expression level of a given gene in a given experiment. Most existing methods for pattern discovery in such matrices are based on clustering genes by comparing their expression levels in all experiments, or clustering experiments by comparing their expression levels for all genes. Our work goes beyond such global approaches by looking for local patterns that manifest themselves when we focus simultaneously on a subset G of the genes and a subset T of the experiments. Specifically, we look for order-preserving submatrices (OPSMs), in which the expression levels of all genes induce the same linear ordering of the experiments (we show that the OPSM search problem is NP-hard in the worst case). Such a pattern might arise, for example, if the experiments in T represent distinct stages in the progress of a disease or in a cellular process, and the expression levels of all genes in G vary across the stages in the same way.We define a probabilistic model in which an OPSM is hidden within an otherwise random matrix. Guided by this model we develop an efficient algorithm for finding the hidden OPSM in the random matrix. In data generated according to the model the algorithm recovers the hidden OPSM with very high success rate. Application of the methods to breast cancer data seems to reveal significant local patterns.Our algorithm can be used to discover more than one OPSM within the same data set, even when these OPSMs overlap. It can also be adapted to handle relaxations and extensions of the OPSM condition. For example, we may allow the different rows of G x T to induce similar but not identical orderings of the columns, or we may allow the set T to include more than one representative of each stage of a biological process.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {49–57},
numpages = {9},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565204,
author = {Ben-Dor, Amir and Karp, Richard M. and Schwikowski, Benno and Shamir, Ron},
title = {The restriction scaffold problem},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565204},
doi = {10.1145/565196.565204},
abstract = {Most shotgun sequencing projects undergo a long and costly phase of finishing, in which a partial assembly forms several contigs whose order, orientation and relative distance is unknown. We propose here a new technique that supplements the shotgun assembly data by cheap and simple complete restriction digests of the target. By computationally combining information from the contig sequences and the fragment sizes measured for several different enzymes, we seek to form a "scaffold" on which the contigs will be placed in their correct orientation, order and distance. We give a heuristic search algorithm for solving the problem and report on promising preliminary simulation results. The key to the success of the search scheme is the very rapid solution of its two time-critical subproblems that are solved precisely in linear time.Our simulations indicate that with noise levels of some 3\% relative error in measuring fragment sizes, using five enzymes, most datasets of 20 contigs can be correctly ordered, and the remaining ones have most of their pairs of neighboring contigs correct. Hence, the technique has a potential to provide real help to finishing. Even when the target clone remains unfinished, the ability to order and orient the contigs correctly makes the partial assembly both more accessible and more useful for biologists.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {58–66},
numpages = {9},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565205,
author = {B\'{e}rard, S\`{e}verine and Rivals, \'{E}ric},
title = {Comparison of minisatellites},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565205},
doi = {10.1145/565196.565205},
abstract = {In the class of repeated sequences that occur in DNA, minisatellites have been found polymorphic and became useful tools in genetic mapping and forensic studies. They consist of a heterogeneous tandem array of a short repeat unit. The slightly different units along the array are called variants. Minisatellites evolve mainly through tandem duplications and tandem deletions of variants. Jeffreys et al. devised a method to obtain the sequence of variants along the array in a digital code, and called such sequences maps. Minisatellite maps give access to the detail of mutation processes at work on such loci. In this paper, we design an algorithm to compare two maps under an evolutionary model that includes deletion, insertion, mutation, tandem duplication and tandem deletion of a variant. Our method computes an optimal alignment in reasonable time; and the alignment score, i.e., the weighted sum of its elementary operations, is a distance metric between maps. The main difficulty is that the optimal sequence of operations depends on the order in which they are applied to the map. Taking the maps of the minisatellite MSY1 of 609 men, we computed all pairwise distances and reconstruct an evolutionary tree of these individuals. MSY1 (DYF155S1) is a hypervariable locus on the Y chromosome. In our tree, the populations of some haplogroups are monophyletic, showing that one can decipher a micro-evolutionary signal using minisatellite maps comparison.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {67–76},
numpages = {10},
keywords = {alignment, bioinformatics, dynamic programming, evolution, minisatellite, overlap graphs, sequence comparison, tandem repeats},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565206,
author = {Bradley, Phil and Kim, Peter S. and Berger, Bonnie},
title = {Trilogy: discovery of sequence-structure patterns across diverse proteins},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565206},
doi = {10.1145/565196.565206},
abstract = {We describe a new computer program, Trilogy, for the automated discovery of sequence-structure patterns in proteins. Trilogy implements a pattern discovery algorithm that begins with an exhaustive analysis of flexible three-residue patterns; a subset of these patterns are selected as seeds for an extension process in which longer patterns are identified. A key feature of the method is explicit treatment of both the sequence and structure components of these motifs: each Trilogy pattern is a pair consisting of a sequence pattern and a structure pattern. Matches to both these component patterns are identified independently, allowing the program to assign a significance score to each sequence-structure pattern that assesses the degree of correlation between the corresponding sequence and structure motifs. Trilogy identifies several thousand high-scoring patterns that occur across protein families. These include both previously identified and novel motifs. We expect that these sequence-structure patterns will be useful in predicting protein structure from sequence, annotating newly determined protein structures, and identifying novel motifs of potential functional or structural significance.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {77–88},
numpages = {12},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565207,
author = {Brivanlou, Ali H.},
title = {Microarray analysis of vertebrate embryonic neural induction},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565207},
doi = {10.1145/565196.565207},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {89},
numpages = {1},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565208,
author = {Buhler, Jeremy},
title = {Provably sensitive Indexing strategies for biosequence similarity search},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565208},
doi = {10.1145/565196.565208},
abstract = {The field of algorithms for pairwise biosequence similarity search is dominated by heuristic methods of high efficiency but uncertain sensitivity. One reason that more formal string matching algorithms with sensitivity guarantees have not been applied to biosequences is that they cannot directly find similarities that score highly under substitution score functions such as the DNAPAM-TT [20], PAM [9], or BLOSUM [12] families of matrices. We describe a general technique, score simulation, to map ungapped similarity search problems using these score functions into the problem of finding pairs of strings that are close in Hamming space. Score simulation leads to indexing schemes for biosequences that permit efficient ungapped similarity searches with formal guarantees of sensitivity using arbitrary score functions. In particular, we introduce the lsh-all-pairs-sim algorithm for finding local similarities in large biosequence collections and show that it is both computationally feasible and sensitive in practice.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {90–99},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565209,
author = {Caprara, Alberto and Lancia, Giuseppe},
title = {Structural alignment of large—size proteins via lagrangian relaxation},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565209},
doi = {10.1145/565196.565209},
abstract = {We illustrate a new approach to the Contact Map Overlap problem for the comparison of protein structures. The approach is based on formulating the problem as an integer linear program and then relaxing in a Lagrangian way a suitable set of constraints. This relaxation is solved by computing a sequence of simple alignment problems, each in quadratic time, and near--optimal Lagrangian multipliers are found by subgradient optimization. By our approach we achieved a substantial speedup over the best existing methods. We were able to solve optimally for the first time instances for PDB proteins with about 1000 residues and 2000 contacts. Moreover, within a few hours we compared 780 pairs in a testbed of 40 large proteins, finding the optimal solution in 150 cases. Finally, we compared 10,000 pairs of proteins from a test set of 269 proteins in the literature, which took a couple of days on a PC.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {100–108},
numpages = {9},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565210,
author = {Cohen, Barry and Skiena, Steven},
title = {Designing RNA structures: natural and artificial selection},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565210},
doi = {10.1145/565196.565210},
abstract = {Messenger RNA (mRNA) sequences serve as templates for proteins according to the triplet code, in which each of the 43 = 64 different codons (sequences of three consecutive nucleotide bases) in RNA either terminate transcription or map to one of the 20 different amino acids (or residues) which build up proteins. Because there are more codons than residues, there is inherent redundancy in the coding. Certain residues (e.g. tryptophan) have only a single corresponding codon, while other residues (e.g. arginine) have as many as six corresponding codons. This freedom implies that the number of possible RNA sequences coding for a given protein grows exponentially in the length of the protein.Thus nature has wide latitude to select among mRNA sequences which are informationally equivalent, but structurally and energetically divergent. In this paper, we explore how nature takes advantage of this freedom, and how to algorithmically design structures more energetically favorable than have been built through natural selection. In particular:Natural Selection, -- We perform the first large-scale computational experiment comparing the stability of mRNA sequences from a variety of organisms to random synonymous sequences which respect the codon preferences of the organism. This experiment was conducted on over 27,000 sequences from 34 microbial species with 36 genomic structures. We provide evidence that in all genomic structures highly stable sequences are disproportionately abundant, and in 19 of 36 cases highly unstable sequences are disproportionately abundant. This suggests that the stability of mRNA sequences is subject to natural selection.Artificial Selection -- Motivated by these biological results, we examine the algorithmic problem of designing the most stable and unstable mRNA sequences which code for a target protein. We give a polynomial-time dynamic programming solution to the most stable sequence problem (MSSP), which is asymptotically no more complex than secondary structure prediction. We show that the corresponding least stable sequence problem (LSSP) is NP-complete, and develop two heuristics for the construction of such sequences.We have implemented these algorithms, and present experimental results placing the high/low stability sequences in context with both wildtype and random encodings. Our implementation has already been applied to the design of RNA "code-words" creating little or no secondary structure in RNA computing [1, 12], and we anticipate a variety of other applications of this work to sequence design problems [16].},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {109–116},
numpages = {8},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565211,
author = {Deng, Minghua and Mehta, Shipra and Sun, Fengzhu and Chen, Ting},
title = {Inferring domain-domain interactions from protein-protein interactions},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565211},
doi = {10.1145/565196.565211},
abstract = {Protein-protein interactions are important events in cellular and biochemical processes within a cell. Several researchers have undertaken the task of analyzing protein-protein interactions covering all genes of an organism by using yeast two-hybrid assays. Protein-protein interactions involve physical interactions between protein domains. Therefore, understanding protein interactions at the domain level gives a global view of the protein interaction network, and possibly extends functions of proteins. In this study, we present a Maximum Likelihood approach to infer domain-domain interactions from the 5719 yeast protein-protein interactions obtained in the high throughput two-hybrid experiments by Uetz et al., 2000 and Ito et al., 2001. The accuracies of our predictions are measured at the protein level. Our study includes the following three results: (1) using the inferred domain-domain interactions, we predict interactions between proteins and achieve 39.0\% specificity and 79.7\% sensitivity; (2) our predicted protein-protein interactions have a significant overlap with the MIPS(http://mips.gfs.de) protein-protein interactions obtained by methods other than the two-hybrid systems; and (3) the mean correlation coefficient of the gene expression profiles for our predicted interacting pairs is significantly higher than that for random pairs as well as that of interacting pairs in Uetz's and Ito's experimental data. Our method has shown robustness in analyzing incomplete data sets and dealing with various experimental errors. We find several novel protein-protein interactions such as RPS0A interacting with APG17 and TAF40 interacting with SPT3, which are consistent with the functions of the proteins.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {117–126},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565212,
author = {Ding, Chris H. Q.},
title = {Analysis of gene expression profiles: class discovery and leaf ordering},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565212},
doi = {10.1145/565196.565212},
abstract = {We approach the class discovery and leaf ordering problems using spectral graph partitioning methodologies. For class discovery or clustering, we present a min-max cut hierarchical clustering method and show it produces subtypes quite close to human expert labeling on the lymphoma dataset with 6 classes. On optimal leaf ordering for displaying the gene expression data, we present a sequential ordering method that can be computed in O(n2) time which also preserves the cluster structure. We also show that the well known statistic methods such as F-statistic test and the principal component analysis are very useful in gene expression analysis.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {127–136},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565213,
author = {Dror, Ron O. and Murnick, Jonathan G. and Rinaldi, Nicola A. and Marinescu, Voichita D. and Rifkin, Ryan M. and Young, Richard A.},
title = {A bayesian approach to transcript estimation from gene array data: the BEAM technique},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565213},
doi = {10.1145/565196.565213},
abstract = {We present a new statistically optimal approach to estimate transcript levels and ratios from one or more gene array experiments. The Bayesian Estimation of Array Measurements (BEAM) technique uses a model of measurement noise and prior information to estimate biological expression levels. It provides a principled method to deal with negative expression level measurements, combine multiple measurements, and identify changes in expression level. BEAM is more flexible than existing techniques, because it does not assume a specific functional form for noise and prior models. Rather, it uses a more accurate noise model developed from experimental data, a process we illustrate here using Affymetrix yeast chips.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {137–143},
numpages = {7},
keywords = {DNA microarrays, affymetrix chips, bayesian estimation, statistical confidence},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565214,
author = {Durand, Dannie and Sankoff, David},
title = {Tests for gene clustering},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565214},
doi = {10.1145/565196.565214},
abstract = {Comparing chromosomal gene order in two or more related species is an important approach to studying the forces that guide genome organization and evolution. Linked clusters of similar genes found in related genomes are often used to support arguments of evolutionary relatedness or functional selection. However, as the gene order and the gene complement of sister genomes diverge progressively due to large scale rearrangements, horizontal gene transfer, gene duplication and gene loss, it becomes increasingly difficult to determine whether observed similarities in local genomic structure are indeed remnants of common ancestral gene order, or are merely coincidences.A rigorous comparative genomics requires principled methods for distinguishing chance commonalities, within or between genomes, from genuine historical or functional relationships. In this paper, we construct tests for significant groupings against null hypotheses of random gene order, taking incomplete clusters, multiple genomes and gene families into account. We consider both the significance of individual clusters of pre-specified genes, and the overall degree of clustering in whole genomes.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {144–154},
numpages = {11},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565215,
author = {Eichler, Evan E.},
title = {Recent duplication, evolution and assembly of the human genome},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565215},
doi = {10.1145/565196.565215},
abstract = {It has been estimated that 5\% of the human genome consists of interspersed duplicated material that has arisen over the last 30 million years of evolution. Two categories of recent duplicated segments can be distinguished: segmental duplications between non-homologous chromosomes (transchromosomal duplications) and duplications largely restricted to a particular chromosome (chromosome-specific duplications). A large proportion of these duplications exhibit an extraordinarily high degree of sequence identity at the nucleotide level (&gt;95\%) spanning large (1--100 kb) genomic distances. Through processes of paralogous recombination, these same regions are targets for rapid evolutionary turnover among the genomes of closely related primates. The dynamic nature of these regions in terms of recurrent chromosomal structural rearrangement and their ability to create fusion genes from juxtaposed cassettes suggests that duplicative transposition has been an important force in the evolution of our genome. Cycles of segmental duplication over periods of evolutionary time may provide the underlying mechanism for domain accretion and the increased modular complexity of the vertebrate proteome. Further, our data suggest that a small fraction of important human genes may have emerged recently through duplication processes and will not possess definitive orthologues in the genomes of model organisms. I will discuss computational methods developed in my laboratory to 1) unambiguously identify recent genomic duplicates within the human genome and 2) to assess their importance in hominoid gene innovation. The impact of this chromosomal architecture for assembly of the final draft sequence will be discussed.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {155},
numpages = {1},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565216,
author = {El-Mabrouk, Nadia and Raffinot, Mathieu},
title = {Approximate matching of secondary structures},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565216},
doi = {10.1145/565196.565216},
abstract = {Several methods have been developed for identifying more or less complex RNA structures in a genome. Whatever the method is, it is always based on the search of conserved primary and secondary structures. While various efficient methods have been developed for searching motifs of the primary structure, usually represented as regular expressions, few effort has been expended in the efficient search of secondary structure signals. By a helix, we mean a structure defined by a combination of sequence and folding constraints. We present a flexible algorithm that searches for all approximate matches of a helix in a genome. Helices are represented by special regular expressions, that we call secondary expressions. The method is based on an alignment graph constructed from several copies of a pushdown automaton, arranged one on top of another. The worst time complexity is O(rpn), where n is the size of the genome, p the size of the secondary expression, and r its number of union symbols. We present our results of searching for specific signals of the tRNA and RNase P RNA in two genomes.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {156–164},
numpages = {9},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565217,
author = {Garner, Harold "Skip"},
title = {Applied computational genomics: polymorphism prediction, data mining and genomic analysis},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565217},
doi = {10.1145/565196.565217},
abstract = {With the "completion" of the first genome, the daily analysis of genomic variation and the transcriptome, and new technologies amassing proteome data, the need for computational tools, inspired by biomedical insight and need are being developed by many groups to help everybody reduce this data to knowledge. To attain understanding from this data, several things are now required for progress; applied computational tools, access to phenotype presenting patients/biological material (and their genotype), and experts to assimilate this information. In this talk, I will describe efforts within our laboratory to develop and then validate some applied computational tools that are then used by our expert collaborators and us. These include: 1) Computer algorithms and codes that inspect the genome (coding and intronic) for highly probable simple sequence repeat and single nucleotide polymorphisms; 2) Techniques and applications to better identify information in text databases; 3) Software to integrate data from some of the many valuable public databases with experimental data (expression and polymorphism), and 4) Software to do integrated analysis and interactive visualization of genomic sequence dat.Each of our software applications is applied to biomedical problems to validate their utility and verify their accuracy, which can only be done by returning to the wet laboratory. The biomedical areas under investigation include cancer, cardiac disease, development, inflammation and infection. This approach has led to gene discoveries, phenotype causing polymorphisms and has inspired research directions for which the final answer is not yet in.Bioinformatics tools often can generate many more "leads" than are possible to pursue with existing technology. This has required us to develop some new technologies and laboratory techniques. For example, we have developed a method and apparatus that is in daily use for the synthesis of custom, software-directed high-density oligonucleotide arrays used for re-sequencing, methlyation analysis and comparative genomic hybridization. This device, called DOC (Digital Optical Chemistry), enables us to manufacture "Affymetrix-style" arrays with up to 192,000 features. Other apparatus, such as high-throughput oligo synthesizers, hyperspectral imaging scanners/microscopes and array spotting robots were also created as needed.This presentation will focus on our applied computational tools and the biomedical observations made with them and touch on how some of the hardware technologies have accelerated that process.Much of the software and databases we develop are available for public use via our www site at http://innovation.swmed.edu/. This work is supported by the NIH, the State of Texas and the P.O'B. Montgomery Distinguished Chair.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {165},
numpages = {1},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565218,
author = {Gusfield, Dan},
title = {Haplotyping as perfect phylogeny: conceptual framework and efficient solutions},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565218},
doi = {10.1145/565196.565218},
abstract = {The next high-priority phase of human genomics will involve the development of a full Haplotype Map of the human genome [12]. It will be used in large-scale screens of populations to associate specific haplotypes with specific complex genetic-influenced diseases. A prototype Haplotype Mapping strategy is presently being finalized by an NIH working-group. The biological key to that strategy is the surprising fact that genomic DNA can be partitioned into long blocks where genetic recombination has been rare, leading to strikingly fewer distinct haplotypes in the population than previously expected [12, 6, 21, 7].In this paper we explore the algorithmic implications of the key (and now realistic) "no-recombination in long blocks" observation, for the problem of inferring haplotypes in populations. We observe that the no-recombination assumption is very powerful. This assumption, along with the standard population-genetic assumption of infinite sites [23, 14] imposes severe combinatorial constraints on the permitted solutions to the haplotype inference problem, leading to an efficient deterministic algorithm to deduce all features of the permitted haplotype solution(s) that can be known with certainty. The technical key is to view haplotype data as disguised information about paths in an unknown tree, and the haplotype deduction problem as a problem of reconstructing the tree from that path information. This formulation allows us to exploit deep theorems and algorithms from graph and matroid theory to efficiently find one permitted solution to the haplotype problem; it gives a simple test to determine if it is the unique solution; if not, we can implicitly represent the set of all permitted solutions so that each can be efficiently created.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {166–175},
numpages = {10},
keywords = {graph realization, graphic matroid recognition, haplotype inference, perfect phylogeny},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565219,
author = {Halperin, Eran and Halperin, Shay and Hartman, Tzvika and Shamir, Ron},
title = {Handling long targets and errors in sequencing by hybridization},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565219},
doi = {10.1145/565196.565219},
abstract = {Sequencing by hybridization (SBH) is a DNA sequencing technique, in which the sequence is reconstructed using its k-mer content. This content, which is called the spectrum of the sequence, is obtained by hybridization to a universal DNA array. Standard universal arrays contain all k-mers for some fixed k, typically 8 to 10. Currently, in spite of its promise and elegance, SBH is not competitive with standard gel-based sequencing methods. This is due to two main reasons: lack of tools to handle realistic levels of hybridization errors, and an inherent limitation on the length of uniquely reconstructible sequence by standard universal arrays.In this paper we deal with both problems. We introduce a simple polynomial reconstruction algorithm which can be applied to spectra from standard arrays and has provable performance in the presence of both false negative and false positive errors. We also propose a novel design of chips containing universal bases, that differs from the one proposed by Preparata et al. We give a simple algorithm that uses spectra from such chips to reconstruct with high probability random sequences of length lower only by a squared log factor compared to the information theoretic bound. Our algorithm is very robust to errors, and has a provable performance even if there are both false negative and false positive errors. Simulations indicate that its sensitivity to errors is also very small in practice.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {176–185},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565220,
author = {Heath, Samuel A. and Preparata, Franco P. and Young, Joel},
title = {Sequencing by hybridization using direct and reverse cooperating spectra},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565220},
doi = {10.1145/565196.565220},
abstract = {DNA sequencing by hybridization, proposed about a decade ago as an alternative to standard electrophoresis-based sequencing techniques, is relevant not only to sequencing per se but also to diagnostics and therapeutics. The inherent structural inadequacy of traditional probe patterns and well-known hybridization shortcomings had for some time deflated the interest in this approach. Renewed interest for this topic has been generated by our recent discovery of a novel probing scheme whose performance for the first time asymptotically meets the information theory bound. After settling the question of asymptotic performance, due to the sizable volume of potential applications, the research focus has naturally shifted to issues of algorithmic fine tunings aimed at improving the performance "constants". In this paper, we introduce as a figure of merit the nucleotide-per-feature ratio (for a given confidence level), i.e., the length of a reliabily reconstructible (random) sequence divided by the number of probes placed on the sequencing array, regardless of the fabrication technology, and we explore the capabilities offered by the joint use of multiple different arrays in sequencing the same DNA sequence. In particular we consider a probing scheme based on the use of the spectra pertaining to a given probing pattern and to its reversal (referred to here as tandem spectra), and we show analytically and experimentally a performance improvement per unit of microarray area of about 4/3 (in a representative instance) over the conventional single-spectrum approach. The proposed tandem-spectrum reconstruction, in conjunction with a "voting provision" discussed elsewhere (whereby one of two competing alternatives is chosen if the corresponding probes are less numerous in the current prefix of the reconstructive sequence), is the best known technique for sequencing by hybridization.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {186–193},
numpages = {8},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565221,
author = {Ho, David D.},
title = {Lymphocyte turnover in HIV-1 infection and the role of the thymus in SIV infection},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565221},
doi = {10.1145/565196.565221},
abstract = {We investigated the dynamics of lymphocyte turnover in normal subjects as well as in HIV-1-infected patients using a novel labeling technique based on the administration of deuterium-containing glucose. A new mathematical model was also used to interpret the results. This approach provided direct determination of lymphocyte proliferation and death rates. We believe our findings convincingly demonstrate the increased turnover (3-6 fold) of both CD4 and CD8 lymphocytes in HIV-1 infection. Furthermore, we also noted that the heightened T-cell turnover reverted toward normal upon effective combination antiretroviral therapy. Collectively, these data show that the lymphocyte depletion seen in AIDS is not a consequence of a T-cell regenerative failure.We have also assessed the role of thymus in contributing to T lymphocyte homeostasis with and without SIV infection. Upon thymectomy in rhesus macaques, we noted a very gradual decline in the number of recent thymic emigrants as measured by the number of T-cell receptor excision circles. These results suggest that the normal thymic output in monkeys is small, and that recent thymic emigrants typically have a prolonged lifespan. We have also found relatively little impact of thymectomy on the outcome of SIV infection. Again, these findings indicate that thymus does not play a major role in the lymphocyte depletion observed in SIV or HIV infection.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {194},
numpages = {1},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565222,
author = {Keich, Uri and Pevzner, Pavel A.},
title = {Finding motifs in the twilight zone},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565222},
doi = {10.1145/565196.565222},
abstract = {We introduce the notion of a multiprofile and use it for finding subtle motifs in DNA sequences. Multiprofiles generalize the notion of a profile and allow one to detect subtle consensus sequences that escape detection by the standard profiles. Our MULTIPROFILER algorithm outperforms other leading motif finding algorithms in a number of synthetic models. Moreover, it can be shown that in some previously studied motif models, MULTIPROFILER is capable of pushing the performance envelope to its theoretical limits.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {195–204},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565223,
author = {Langmead, Christopher James and Yan, Anthony K. and McClung, C. Robertson and Donald, Bruce Randall},
title = {Phase-independent rhythmic analysis of genome-wide expression patterns},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565223},
doi = {10.1145/565196.565223},
abstract = {We introduce a model-based analysis technique for extracting and characterizing rhythmic expression profiles from genome-wide DNA microarray hybridization data. These patterns are clues to discovering rhythmic genes implicated in cell-cycle, circadian, and other biological processes. The algorithm, implemented in a program called RAGE (Rhythmic Analysis of Gene Expression), decouples the problems of estimating a pattern's periodicity and phase. Our algorithm is linear-time in frequency and phase resolution, an improvement over previous quadratic-time approaches. Unlike previous approaches, RAGE uses a true distance metric for measuring expression profile similarity, based on the Hausdorff distance. This results in better clustering of expression profiles for rhythmic analysis. The confidence of each frequency estimate is computed using Z-scores. We demonstrate that RAGE is superior to other techniques on synthetic and actual DNA microarray hybridization data. We also show how to replace the discretized phase search in our method with an exact (combinatorially precise) phase search, resulting in a faster algorithm with no complexity dependence on phase resolution.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {205–215},
numpages = {11},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565224,
author = {Li, Jia and Miller, Webb},
title = {Significance Of inter-species matches when evolutionary rate varies},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565224},
doi = {10.1145/565196.565224},
abstract = {We develop techniques to estimate the statistical significance of gap-free alignments between two genomic DNA sequences, using human-mouse alignments as an example. The sequences are assumed to be sufficiently similar that some but not all of the neutrally evolving regions (i.e., those under no evolutionary constraint) can be reliably aligned. Our goal is to model the situation in which the neutral rate of evolution, and hence the extent of the aligning intervals, varies across the genome. In some cases, this permits the weaker of two matches to be judged as less likely to have arisen by chance, provided it lies in a genomic interval with a high level of background divergence. We employ a Hidden Markov Model to capture variations in divergence rates, and assign probability values to gap-free alignments using techniques related to those used for the same purpose by Blast. Our methods are illustrated in detail using a 1.49 Mb genomic region. Preliminary results using all of human chromosome 22 indicate that these techniques will work for the entire human genome.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {216–224},
numpages = {9},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565225,
author = {Liao, Li and Noble, William Stafford},
title = {Combining pairwise sequence similarity and support vector machines for remote protein homology detection},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565225},
doi = {10.1145/565196.565225},
abstract = {One key element in understanding the molecular machinery of the cell is to understand the meaning, or function, of each protein encoded in the genome. A very successful means of inferring the function of a previously unannotated protein is via sequence similarity with one or more proteins whose functions are already known. Currently, one of the most powerful such homology detection methods is the SVM-Fisher method of Jaakkola, Diekhans and Haussler (ISMB 2000). This method combines a generative, profile hidden Markov model (HMM) with a discriminative classification algorithm known as a support vector machine (SVM). The current work presents an alternative method for SVM-based protein classification. The method, SVM-pairwise, uses a pairwise sequence similarity algorithm such as Smith-Waterman in place of the HMM in the SVM-Fisher method. The resulting algorithm, when tested on its ability to recognize previously unseen families from the SCOP database, yields significantly better remote protein homology detection than SVM-Fisher, profile HMMs and PSI-BLAST.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {225–232},
numpages = {8},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565226,
author = {Mumey, Brendan M. and Bailey, Brian W. and Dratz, Edward A.},
title = {Revealing protein structures: a new method for mapping antibody epitopes},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565226},
doi = {10.1145/565196.565226},
abstract = {A recent idea for determining the three-dimensional structure of a protein uses antibody recognition of surface structure and random peptide libraries to map antibody epitope combining sites. Antibodies that bind to the surface of the protein of interest can be used as "witnesses" to report the structure of the protein as follows: Proteins are composed of linear polypeptide chains that come together in complex spatial folding patterns to create the native protein structures and these folded structures form the binding sites for the antibodies. Short amino acid probe sequences, which bind to the active region of each antibody, can be selected from random sequence peptide libraries. These probe sequences can often be aligned to discontinuous regions of the one-dimensional target sequence of a protein. Such alignments indicate how pieces of the protein sequence must be folded together in space and thus provide valuable long-range constraints for solving the overall 3-D structure. This new approach is applicable to the very large number of proteins that are refractory to current approaches to structure determination and has the advantage of requiring very small amounts of the target protein. The binding site of an antibody is a surface, not just a linear sequence, so the epitope mapping alignment problem is outside the scope of classical string alignment algorithms, such as Smith-Waterman. We formalize the alignment problem that is at the heart of this new approach, prove that the epitope mapping alignment problem is NP-complete, and give some initial results using a branch-and-bound algorithm to map two real-life cases.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {233–240},
numpages = {8},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565227,
author = {Ott, Jurg and Hoh, Josephine},
title = {Set association analysis of SNP case-control and microarray data},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565227},
doi = {10.1145/565196.565227},
abstract = {Common heritable diseases ("complex traits") are assumed to be due to multiple underlying susceptibility genes. While genetic mapping methods for mendelian disorders have been very successful, the search for genes underlying complex traits has been difficult and often disappointing. One of the reasons may be that most current gene mapping approaches are still based on conventional methodology of testing one or a few SNPs at a time. Here we demonstrate a simple strategy that allows for the joint analysis of multiple disease-associated SNPs in different genomic regions. Our set-association method combines information over SNPs by forming sums of relevant single-marker statistics. This approach successfully addresses the "curse of dimensionality" problem - too many variables should be estimated with a comparatively small number of observations. We also extend our method to microarray expression data, where expression levels for large numbers of genes should be compared between two tissue types. In applications to experimental expression data our approach turned out to be highly efficient.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {241–245},
numpages = {5},
keywords = {association, case control study, expression, microarray},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565228,
author = {Pachter, Lior and Lam, Fumei},
title = {Picking alignments from (steiner) trees},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565228},
doi = {10.1145/565196.565228},
abstract = {The application of Needleman-Wunsch alignment techniques to biological sequences is complicated by two serious problems when the sequences are long: the running time, which scales as the product of the lengths of sequences, and the difficulty in obtaining suitable parameters that produce meaningful alignments. The running time problem is often corrected by reducing the search space, using techniques such as banding, or chaining of high scoring pairs. The parameter problem is more difficult to fix, partly because the probabilistic model, which Needleman-Wunsch is equivalent to, does not capture a key feature of biological sequence alignments, namely the alternation of conserved blocks and seemingly unrelated non-conserved segments. We present a solution to the problem of designing efficient search spaces for pair hidden Markov models that align biological sequences by taking advantage of their associated features. Our approach leads to an optimization problem, for which we obtain a 2-approximation algorithm, and that is based on the construction of Manhattan networks, which are close relatives of Steiner trees. We describe the underlying theory and show how our methods can be applied to alignment of DNA sequences in practice, succesfully reducing the Viterbi algorithm search space of alignment PHMMs by three orders of magnitude.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {246–253},
numpages = {8},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565229,
author = {Rash, Sam and Gusfield, Dan},
title = {String barcoding: uncovering optimal virus signatures},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565229},
doi = {10.1145/565196.565229},
abstract = {There are many critical situations when one needs to rapidly identify an unidentified pathogen from among a given set of previously sequenced pathogens. DNA or RNA hybridization chips can be designed for such identifications. Each cell in the chip can report the presence or absence of a specific substring of DNA in the unidentified pathogen. Properly designed, the collection of reports obtained from the cells can uniquely identify any pathogen in the set, or determine that the unidentified pathogen is not in the set. There is a limit to the number of cells on a chip, and a range of substring lengths that a cell can handle. So, given the full sequences of a set of pathogens, the problem is to design the chip by selecting the smallest set of substrings of the appropriate lengths, so that each pathogen in the set has a unique set of cells that report a substring. For any given pathogen, the set of reporting cells is its signature, and hence the entire system is a "barcode" system for the pathogens.Previous work addressed this problem [1], but focused on pathogens of bacterial size, and hence had to make many compromises for the sake of efficiency. The substrings lengths were severely restricted, and no optimality or near-optimality was guaranteed. In this paper, we focus on viral-size pathogens. We show that for genomes of this size, it is practical to solve the barcode design problem optimally, or near-optimally, without artificially constraining the problem. We also efficiently find barcodes that provide a level of redundancy, tolerating a number of errors or mutations. The key technical ideas are the use of suffix trees to identify the critical substrings, integer-linear programming (ILP) to express the minimization problem, and a simple idea that dramatically reduces the size of the ILP, allowing it to be solved efficiently by the commercial ILP solver CPLEX. We report extensive tests of our approach on various collections of virus DNA and RNA sequences.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {254–261},
numpages = {8},
keywords = {barcoding, string barcoding, suffix trees, testing set, virus signatures},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565230,
author = {Rubin, Gerald M.},
title = {Biological and computational annotation of the Drosophila Genome Sequence},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565230},
doi = {10.1145/565196.565230},
abstract = {The nucleotide sequence of the Drosophila melanogaster genome is now available and efforts are ongoing to improve its accuracy and annotation. The value of these sequence data will be enormously enhanced if we can provide for each gene information on: (1) the structure and expression pattern of its transcripts; (2) the mechanisms and logic used to control their expression; and (3) the functions of their protein products. My presentation will be directed toward the challenges of obtaining and interpreting such information for the fruit fly. Gene sequence and expression pattern databases will be extremely powerful tools. However, the function of a protein in a multicellular organism depends on context and will almost certainly need to be determined by experimental analysis. Collection of the required large datasets will be difficult and neither the intellectual framework nor experimental tools for analyzing complex gene networks are currently in place. Nevertheless, there is reason for cautious optimism that the complete genomic sequence of organisms will enable the necessary global approaches to study gene function and regulation. The conservation of gene structure and function during evolution will allow for the linking and sharing of information garnered in different experimental systems. But what data should be collected and how to interpret these data are much less clear. I will describe recent attempts by the Berkeley Drosophila Genome project to address some of these issues.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {262},
numpages = {1},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565231,
author = {Segal, Eran and Barash, Yoseph and Simon, Itamar and Friedman, Nir and Koller, Daphne},
title = {From promoter sequence to expression: a probabilistic framework},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565231},
doi = {10.1145/565196.565231},
abstract = {We present a probabilistic framework that models the process by which transcriptional binding explains the mRNA expression of different genes. Our joint probabilistic model unifies the two key components of this process: the prediction of gene regulation events from sequence motifs in the gene's promoter region, and the prediction of mRNA expression from combinations of gene regulation events in different settings. Our approach has several advantages. By learning promoter sequence motifs that are directly predictive of expression data, it can improve the identification of binding site patterns. It is also able to identify combinatorial regulation via interactions of different transcription factors. Finally, the general framework allows us to integrate additional data sources, including data from the recent binding localization assays. We demonstrate our approach on the cell cycle data of Spellman et al., combined with the binding localization information of Simon et al. We show that the learned model predicts expression from sequence, and that it identifies coherent co-regulated groups with significant transcription factor motifs. It also provides valuable biological insight into the domain via these co-regulated "modules" and the combinatorial regulation effects that govern their behavior.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {263–272},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565232,
author = {Segal, Eran and Koller, Daphne},
title = {Probabilistic hierarchical clustering for biological data},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565232},
doi = {10.1145/565196.565232},
abstract = {Biological data, such as gene expression profiles or protein sequences, is often organized in a hierarchy of classes, where the instances assigned to "nearby" classes in the tree are similar. Most approaches for constructing a hierarchy use simple local operations, that are very sensitive to noise or variation in the data. In this paper, we describe probabilistic abstraction hierarchies (PAH) [11], a general probabilistic framework for clustering data into a hierarchy, and show how it can be applied to a wide variety of biological data sets. In a PAH, each class is associated with a probabilistic generative model for the data in the class. The PAH clustering algorithm simultaneously optimizes three things: the assignment of data instances to clusters, the models associated with the clusters, and the structure of the PAH approach is that it utilizes global optimization algorithms for the last two steps, substantially reducing the sensitivity to noise and the propensity to local maxima. We show how to apply this framework to gene expression data, protein sequence data, and HIV protease sequence data. We also show how our framework supports hierarchies involving more than one type of data. We demonstrate that our method extracts useful biological knowledge and is substantially more robust than hierarchical agglomerative clustering.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {273–280},
numpages = {8},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565233,
author = {Siepel, Adam C.},
title = {An algorithm to enumerate all sorting reversals},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565233},
doi = {10.1145/565196.565233},
abstract = {The problem of estimating evolutionary distance from differences in gene order has been distilled to the problem of finding the reversal distance between two signed permutations. During the last decade, much progress was made both in computing reversal distance and in finding a minimum sequence of sorting reversals. For most problem instances, however, many minimum sequences of sorting reversals exist, and obtaining the complete set can be useful in exploring the space of genome rearrangements (e.g., in pursuit of solutions to higher-level problems). The problem of finding all minimum sequences of sorting reversals reduces easily to the problem of finding all sorting reversals of one permutation with respect to another. We derive an efficient algorithm to solve this latter problem, and present experimental results indicating that our algorithm offers a dramatic improvement over the best known alternative. It should be noted that in asymptotic terms the new algorithm does not represent a significant improvement: it requires O(n3) time (where n is the permutation size), while the problem can now be solved trivially in Θ(n3) time.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {281–290},
numpages = {10},
keywords = {genome rearrangements, sorting by reversals},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565234,
author = {Sinha, Saurabh},
title = {Discriminative motifs},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565234},
doi = {10.1145/565196.565234},
abstract = {This paper takes a new view of motif discovery, addressing a common problem in existing motif finders. A motif is treated as a feature of the input promoter regions that leads to a good classifier between these promoters and a set of background promoters. This perspective allows us to adapt existing methods of feature selection, a well studied topic in machine learning, to motif discovery. We develop a general algorithmic framework that can be specialized to work with a wide variety of motif models, including consensus models with degenerate symbols or mismatches, and composite motifs. A key feature of our algorithm is that it measures over-representation while maintaining information about the distribution of motif instances in individual promoters. The assessment of a motif's discriminative power is normalized against chance behaviour by a probabilistic analysis. We apply our framework to two popular motif models, and are able to detect several known binding sites in sets of co-regulated genes in yeast.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {291–298},
numpages = {8},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565235,
author = {Teodoro, Miguel L. and Phillips, George N. and Kavraki, Lydia E.},
title = {A dimensionality reduction approach to modeling protein flexibility},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565235},
doi = {10.1145/565196.565235},
abstract = {Proteins are involved either directly or indirectly in all biological processes in living organisms. It is now widely accepted that conformational changes of proteins can critically affect their ability to bind other molecules and that any progress in modeling protein motion and flexibility will contribute to the understanding of key biological functions. However, modeling protein flexibility has proven a very difficult task. Experimental laboratory methods such as X-ray crystallography produce rather few structures, while computational methods such as Molecular Dynamics are too slow for routine use with large systems. A medium sized protein typically has a few thousands of degrees of freedom. This paper shows how to obtain a reduced basis representation of protein flexibility. We use the Principal Component Analysis method, a dimensionality reduction technique, to transform the original high dimensional representation of protein motion into a lower dimensional representation that captures the dominant modes of motions of the protein. Although there is inevitably some loss in accuracy, we show that we can obtain conformations that have been observed in laboratory experiments, starting from different initial conformations and working in a drastically reduced search space.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {299–308},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565236,
author = {Venter, J. Craig},
title = {Sequencing the human genome},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565236},
doi = {10.1145/565196.565236},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {309},
numpages = {1},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565237,
author = {Vidal, Marc},
title = {Toward a proteome atlas for C. Elegans},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565237},
doi = {10.1145/565196.565237},
abstract = {The availability of complete genome sequences suggests new approaches for biological research to complement conventional genetics and biochemistry. In this context, the goals of this laboratory are to generate a comprehensive protein-protein interaction, or interactome, map for C. elegans and develop new concepts to integrate this map with other functions such as expression profiles (transcriptome), global phenotypic analysis (phenome), localization of expression projects localizome, etc... The resulting atlas of integrated maps should be valuable for the development of a systems biology approach to the study of development.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {310},
numpages = {1},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565238,
author = {Wildenberg, Andy and Skiena, Steven and Sumazin, Pavel},
title = {Deconvolving sequence variation in mixed DNA populations},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565238},
doi = {10.1145/565196.565238},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {311–320},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

@inproceedings{10.1145/565196.565239,
author = {Zien, Alexander and Fluck, Juliane and Zimmer, Ralf and Lengauer, Thomas},
title = {Microarrays: how many do you need?},
year = {2002},
isbn = {1581134983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/565196.565239},
doi = {10.1145/565196.565239},
abstract = {We estimate the number of microarrays that is required in order to gain reliable results from a common type of study: the pairwise comparison of different classes of samples. Current knowlegde seems to suffice for the construction of models that are realistic with respect to searches for individual differentially expressed genes. Such models allow to investigate the dependence of the required number of samples on the relevant parameters: the biological variability of the samples within each class; the fold changes in expression; the detection sensitivity of the microarrays; and the acceptable error rates of the results. We supply experimentalists with general conclusions as well as a freely accessible Java applet at http://cartan.gmd.de/~zien/classsize/ for fine tuning simulations to their particular actualities. Since the situation can be assumed to be very similar for large scale proteomics and metabolomics studies, our methods and results might also apply there.},
booktitle = {Proceedings of the Sixth Annual International Conference on Computational Biology},
pages = {321–330},
numpages = {10},
location = {Washington, DC, USA},
series = {RECOMB '02}
}

