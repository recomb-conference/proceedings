@inproceedings{10.1145/640075.640076,
author = {Akutsu, Tatsuya},
title = {Efficient extraction of mapping rules of atoms from enzymatic reaction data},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640076},
doi = {10.1145/640075.640076},
abstract = {Extraction of mapping rules of atoms from enzymatic reaction data is useful for drug design, simulation of tracer experiments and consistency checking of pathway databases. Most of previous methods for this problem are based on maximal common subgraph algorithms. In this paper, we propose a novel approach based on graph partition and graph isomorphism. We show that this problem is NP-hard in general, but can be solved in polynomial time for wide classes of enzymatic reactions. We also present an O(n1.5) time algorithm for a special but fundamental class of reactions, where n is the maximum size of compounds appearing in a reaction. We develop practical polynomial time algorithms in which the Morgan algorithm is used for computing the normal form of a graph, where it is known that the Morgan algorithm works correctly for most chemical structures. Computational experiments are performed for these practical algorithms using the chemical reaction data stored in the KEGG/LIGAND database. The results of computational experiments suggest that practical algorithms are useful in many cases.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {1–8},
numpages = {8},
keywords = {metabolic pathways, graph isomorphism, common subgraph, chemical reaction},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640077,
author = {Bafna, Vineet and Edwards, Nathan},
title = {On de novo interpretation of tandem mass spectra for peptide identification},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640077},
doi = {10.1145/640075.640077},
abstract = {The correct interpretation of tandem mass spectra is a difficult problem, even when it is limited to scoring peptides against a database. De novo sequencing is considerably harder, but critical when sequence databases are incomplete or not available. In this paper we build upon earlier work due to Dancik et al., and Chen et al. to provide a dynamic programming algorithm for interpreting de novo spectra. Our method can handle most of the commonly occurring ions, including a; b; y, and their neutral losses. Additionally, we shift the emphasis away from sequencing to assigning ion types to peaks. In particular, we introduce the notion of core interpretations, which allow us to give confidence values to individual peak assignments, even in the absence of a strong interpretation. Finally, we introduce a systematic approach to evaluating de novo algorithms as a function of spectral quality. We show that our algorithm, in particular the core-interpretation, is robust in the presence of measurement error, and low fragmentation probability.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {9–18},
numpages = {10},
keywords = {de novo interpretation, proteomics, tandem mass spectrometry},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640078,
author = {Bafna, Vineet and Halldorsson, Bjarni V. and Schwartz, Russell and Clark, Andrew G. and Istrail, Sorin},
title = {Haplotypes and informative SNP selection algorithms: don't block out information},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640078},
doi = {10.1145/640075.640078},
abstract = {It is widely hoped that variation in the human genome will provide a means of predicting risk of a variety of complex, chronic diseases. A major stumbling block to the successful identification of association between human DNA polymorphisms (SNPs) and variability in risk of complex diseases is the enormous number of SNPs in the human genome (4,9). The large number of SNPs results in unacceptably high costs for exhaustive genotyping, and so there is a broad effort to determine ways to select SNPs so as to maximize the informativeness of a subset.In this paper we contrast two methods for reducing the complexity of SNP variation: haplotype tagging, i.e. typing a subset of SNPs to identify segments of the genome that appear to be nearly unrecombined (haplotype blocks), and a new block-free model that we develop in this report. We present a statistic for comparing haplotype blocks and show that while the concept of haplotype blocks is reasonably robust there is substantial variability among block partitions. We develop a measure for selecting an informative subset of SNPs in a block free model. We show that the general version of this problem is NP-hard and give efficient algorithms for two important special cases of this problem.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {19–27},
numpages = {9},
keywords = {SNPs, haplotype blocks, haplotype tagging},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640079,
author = {Barash, Yoseph and Elidan, Gal and Friedman, Nir and Kaplan, Tommy},
title = {Modeling dependencies in protein-DNA binding sites},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640079},
doi = {10.1145/640075.640079},
abstract = {The availability of whole genome sequences and high-throughput genomic assays opens the door for in silico analysis of transcription regulation. This includes methods for discovering and characterizing the binding sites of DNA-binding proteins, such as transcription factors. A common representation of transcription factor binding sites is a position specific score matrix (PSSM). This representation makes the strong assumption that binding site positions are independent of each other. In this work, we explore Bayesian network representations of binding sites that provide different tradeoffs between complexity (number of parameters) and the richness of dependencies between positions. We develop the formal machinery for learning such models from data and for estimating the statistical significance of putative binding sites. We then evaluate the ramifications of these richer representations in characterizing binding site motifs and predicting their genomic locations. We show that these richer representations improve over the PSSM model in both tasks.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {28–37},
numpages = {10},
keywords = {DNA sequence motifs, bayesian networks, factors binding sites, transcription},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640080,
author = {Bejerano, Gill},
title = {Efficient exact value computation and applications to biosequence analysis},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640080},
doi = {10.1145/640075.640080},
abstract = {Like other fields of life sciences, bioinformatics has turned to capture biological phenomena through probabilistic models, and to analyse these models using statistical methodology. A central computational problem in applying useful statistical procedures such as various hypothesis testing procedures is the computation of p-values. In this paper, we devise a branch and bound approach to efficient exact p-value computation, and apply it to a likelihood ratio test in a frequency table setting. By recursively partitioning the sample domain and bounding the statistic we avoid the explicit exhaustive enumeration of all possible outcomes which is currently carried by the standard statistical packages. The convexity of the test statistic is further utilized to confer additional speed-up.Empirical evaluation demonstrates a reduction in the computational complexity of the algorithm, even in worst case scenarios, significantly extending the practical range for performing the exact test. We also show that speed-up greatly improves the sparser the underlying null hypothesis is; that computation precision actually increases with speed-up; and that computation time is very moderately affected by the magnitude of the computed p-value. These qualities make our algorithm an appealing alternative to the exhaustive test, the Χ2 asymptotic approximation and Monte Carlo samplers in the respective regimes.The proposed method is readily extendible to other tests and test statistics of interest. We survey several examples of established biosequence analysis methods, where small sample size and sparseness do occur, and to which our computational framework could be applied to improve performance. We briefly demonstrate this with two applications, measuring binding site positional correlations in DNA, and detecting compensatory mutation events in functional RNA.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {38–47},
numpages = {10},
keywords = {p-value, branch and bound, categorical data, exact test, frequency tables, real extension},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640081,
author = {Ben-Dor, Amir and Hartman, Tzvika and Schwikowski, Benno and Sharan, Roded and Yakhini, Zohar},
title = {Towards optimally multiplexed applications of universal DNA tag systems},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640081},
doi = {10.1145/640075.640081},
abstract = {We study a design and optimization problem that occurs, for example, when single nucleotide polymorphisms (SNPs) are to be genotyped using a universal DNA tag array. The problem of optimizing the universal array to avoid disruptive cross-hybridization between universal components of the system was addressed in a previous work. However, cross-hybridization can also occur assay-specifically, due to unwanted complementarity involving assay-specific components. Here we examine the problem of identifying the most economic experimental configuration of the assay-specific components that avoids cross-hybridization. Our formalization translates this problem into the problem of covering the vertices of one side of a bipartite graph by a minimum number of balanced subgraphs of maximum degree 1. We show that the general problem is NP-complete. However, in the real biological setting the vertices that need to be covered have degrees bounded by d. We exploit this restriction and develop an O(d)-approximation algorithm for the problem. We also give an O(d)-approximation for a variant of the problem in which the covering subgraphs are required to be vertex-disjoint. In addition, we propose a stochastic model for the input data and use it to prove a lower bound on the cover size. We complement our theoretical analysis by implementing two heuristic approaches and testing their performance on simulated and real SNP data.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {48–56},
numpages = {9},
keywords = {universal array, stochastic model, minimum primer cover, cross-hybridization, SNP genotyping},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640082,
author = {Blanchette, Mathieu},
title = {A comparative analysis method for detecting binding sites in coding regions},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640082},
doi = {10.1145/640075.640082},
abstract = {While the problem of predicting transcription factor binding sites in a gene's promoter region has been extensively studied, binding sites located in coding regions are also crucial for regulating gene expression but are more difficult to detect. Coding region binding sites are mostly involved in splicing regulation, but also in transcriptional and post-transcriptional regulation. We consider the problem of predicting such binding sites by comparative analysis. Comparative analysis is based on the idea that functional sequences tend to evolve at slower rate than nonfunctional sequence, making unusually well conserved regions likely to be of interest. The difficulty in applying comparative analysis to the detection of binding sites located in coding sequence is that the whole sequence is under selective pressure, because it needs to code for a functional protein. We present a technique to distinguish between conservation due to constraints on the amino acid product and conservation due to constraints imposed by regulatory factors. More precisely, we show how to calculate the probability of observing a certain degree of conservation among the nucleotides of given set of orthologous codons, given a set of constraints on the amino acids they need to encode. The algorithms described are implemented in a program called Cosmo, available at http://bio.cs.washington.edu. We ran Cosmo on several genes known to contain exonic splicing enhancers and report the results.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {57–66},
numpages = {10},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640083,
author = {Buhler, Jeremy and Keich, Uri and Sun, Yanni},
title = {Designing seeds for similarity search in genomic DNA},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640083},
doi = {10.1145/640075.640083},
abstract = {Large-scale comparison of genomic DNA is of fundamental importance in annotating functional elements of genomes. To perform large comparisons e.ciently, BLAST [3, 2] and other widely used tools use seeded alignment, which compares only sequences that can be shown to share a common pattern or "seed" of matching bases. The literature suggests that the choice of seed substantially affects the sensitivity of seeded alignment, but designing and evaluating seeds is computationally challenging. This work addresses problems arising in seed design. We give the fastest known algorithm for evaluating the sensitivity of a seed in a Markov model of ungapped alignments, as well as theoretical results on which seeds are good choices. We also describe Mandala, a software tool for seed design, and show that it can be used to improve the sensitivity of alignment in practice.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {67–75},
numpages = {9},
keywords = {Mandala, biosequence comparison, genomic DNA, seed design, similarity search},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640084,
author = {Chor, Benny and Khetan, Amit and Snir, Sagi},
title = {Maximum likelihood on four taxa phylogenetic trees: analytic solutions},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640084},
doi = {10.1145/640075.640084},
abstract = {Maximum likelihood (ML) is increasingly used as an optimality criterion for selecting evolutionary trees (Felsenstein, 1981), but finding the global optimum is a hard computational task. Because no general analytic solution is known, numeric techniques such as hill climbing or expectation maximization (EM), are used in order to find optimal parameters for a given tree. So far, analytic solutions were derived only for the simplest model - three taxa, two state characters, under a molecular clock (MC). Quoting Ziheng Yang (2000), who initiated the analytic approach, "this seems to be the simplest case, but has many of the conceptual and statistical complexities involved in phylogenetic estimation".In this work, we give analytic solutions for four taxa, two state characters under a molecular clock. The change from three to four taxa incurs a major increase in the complexity of the underlying algebraic system, and requires novel techniques and approaches. We start by presenting the general maximum likelihood problem on phylogenetic trees as a constrained optimization problem, and the resulting system of polynomial equations. In full generality, it is infeasible to solve this system, therefore specialized tools for the MC case are developed.Four taxa rooted trees have two topologies -- the fork (two subtrees with two leaves each) and the comb (one subtree with three leaves, the other with a single leaf). We combine the ultrametric properties of MC trees with the Hadamard conjugation (Hendy and Penny, 1993) to derive a number of topology dependent identities. Employing these identities, we substantially simplify the system of polynomial equations. We finally use tools from algebraic geometry (e.g. Grobner bases, ideal saturation, resultants) and employ symbolic algebra software to obtain closed form analytic solutions (expressed parametrically in the input data) for the fork topology, and analytic solutions for the comb. We show that in contrast to the fork, the comb has no closed form solutions (expressed by radicals in the input data). In general, four taxa trees can have multiple ML points (Steel, 1994, Chor et. al., 2001). In contrast, we can now prove that under the MC assumption, both the fork and the comb topologies have a unique (local and global) ML point.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {76–83},
numpages = {8},
keywords = {2-state model, Groebner bases, Hadamard conjugation, constrained optimization, maximum likelihood, molecular clock, phylogenetic trees, saturation, symbolic algebra},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640085,
author = {Chiang, Derek Y. and Moses, Alan M. and Kamvysselis, Manolis and Lander, Eric S. and Eisen, Michael B.},
title = {Phylogenetically and spatially conserved word pairs associated with gene expression changes in yeasts},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640085},
doi = {10.1145/640075.640085},
abstract = {Background. Transcriptional regulation in eukaryotes is often multifactorial, involving multiple transcription factors binding to the same transcription control region (e.g., upstream activating sequences and enhancers), and to understand the regulatory content of eukaryotic genomes it is necessary to consider the co-occurrence and spatial relationships of individual binding sites. The identification of sequences conserved among related species (often known as phylogenetic footprinting) has been successfully used to identify individual transcription factor binding sites. Here, we extend this concept of functional conservation to higher-order features of transcription control regions involved in the multifactorial control of gene expression.Results. We used the genome sequences of four yeast species of the genus Saccharomyces to identify sequences potentially involved in multifactorial control of gene expression. We found 1,117 potential regulatory "templates": pairs of hexameric sequences that are jointly conserved in transcription regulatory regions and also exhibit non-random relative spacing. Many of the individual sequences in these templates correspond to known transcription factor binding sites, and the sets of genes containing a particular template in their transcription control regions tend to be differentially expressed in conditions where the corresponding transcription factors are known to be active.Conclusions. The incorporation of both joint conservation and spacing constraints of sequence pairs predicts groups of target genes that were specific for common patterns of gene expression. Our work suggests that positional information, especially the relative spacing between transcription factor binding sites, may represent a common organizing principle of transcription control regions.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {84–93},
numpages = {10},
keywords = {comparative genomics, multifactorial regulation, phylogenetic footprinting, promoter structure, transcription regulation},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640086,
author = {Clark, Andrew G.},
title = {Haplotype phase inference},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640086},
doi = {10.1145/640075.640086},
abstract = {Most of the information being collected on DNA variation among people does not identify which of the two parents transmitted which of the two copies of each gene. Even worse, the parent of origin is often scrambled for each single nucleotide polymorphism (SNP) that is identified, so that each gene may be represented by hundreds of pairs of SNP vectors or haplotypes. Collection of a population sample of this kind of genotype data, however, does contain information about these haplotypes, and inference of the haplotype pairs from this kind of data is referred to has haplotype phase inference. The problem has a rich geometric representation, and has spawned a wealth of algorithms that span graph theoretic to Bayesian approaches. Good solutions to this problem are strongly motivated by the efforts seeking to identify genes that underlie complex genetic disorders. The latest efforts in this area will be described and reviewed.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {94},
numpages = {1},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640087,
author = {Deng, Minghua and Chen, Ting and Sun, Fengzhu},
title = {An integrated probabilistic model for functional prediction of proteins},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640087},
doi = {10.1145/640075.640087},
abstract = {We develop an integrated probabilistic model to combine protein physical interactions, genetic interactions, highly correlated gene expression network, protein complex data, and domain structures of individual proteins to predict protein functions. The model is an extension of our previous model for protein function prediction based on Markovian random field theory. The model is flexible in that other protein pairwise relationship information and features of individual proteins can be easily incorporated. Two features distinguish the integrated approach from other available methods for protein function prediction. One is that the integrated approach uses all available sources of information with different weights for different sources of data. It is a global approach that takes the whole network into consideration. The second feature is that the posterior probability that a protein has the function of interest is assigned. The posterior probability indicates how confident we are about assigning the function to the protein. We apply our integrated approach to predict functions of yeast proteins based upon MIPS protein function classifications and upon the interaction networks based on MIPS physical and genetic interactions, gene expression profiles, Tandem Affinity Purification (TAP) protein complex data, and protein domain information. We study the sensitivity and specificity of the integrated approach using different sources of information by the leave-one-out approach. In contrast to using MIPS physical interactions only, the integrated approach combining all of the information increases the sensitivity from 57\% to 87\% when the specificity is set at 57\%-an increase of 30\%. It should also be noted that enlarging the interaction network greatly increases the number of proteins whose functions can be predicted.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {95–103},
numpages = {9},
keywords = {Gibbs sampler, Markov random field, function prediction, pfam domain, protein-protein interaction},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640088,
author = {Eskin, Eleazar and Halperin, Eran and Karp, Richard M.},
title = {Large scale reconstruction of haplotypes from genotype data},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640088},
doi = {10.1145/640075.640088},
abstract = {Critical to the understanding of the genetic basis for complex diseases is the modeling of human variation. Most of this variation can be characterized by single nucleotide polymorphisms (SNPs) which are mutations at a single nucleotide position. To characterize an individual's variation, we must determine an individual's haplotype or which nucleotide base occurs at each position of these common SNPs for each chromosome. In this paper, we present results for a highly accurate method for haplotype resolution from genotype data. Our method leverages a new insight into the underlying structure of haplotypes which shows that SNPs are organized in highly correlated "blocks". The majority of individuals have one of about four common haplotypes in each block. Our method partitions the SNPs into blocks and for each block, we predict the common haplotypes and each individual's haplotype. We evaluate our method over biological data. Our method predicts the common haplotypes perfectly and has a very low error rate (0.47\%) when taking into account the predictions for the uncommon haplotypes. Our method is extremely efficient compared to previous methods, (a matter of seconds where previous methods needed hours). Its efficiency allows us to find the block partition of the haplotypes, to cope with missing data and to work with large data sets such as genotypes for thousands of SNPs for hundreds of individuals. The algorithm is available via webserver at http://www.cs.columbia.edu/compbio/hap.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {104–113},
numpages = {10},
keywords = {haplotype resolution, perfect phylogeny, single nucleotide polymorphisms (SNPs)},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640089,
author = {Fishelson, Ma\'{a}yan and Geiger, Dan},
title = {Optimizing exact genetic linkage computations},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640089},
doi = {10.1145/640075.640089},
abstract = {Genetic linkage analysis is a challenging application which requires Bayesian networks consisting of thousands of vertices. Consequently, computing the likelihood of data, which is needed for learning linkage parameters, using exact inference procedures calls for an extremely efficient implementation that carefully optimizes the order of conditioning and summation operations. In this paper we present the use of stochastic greedy algorithms for optimizing this order. Our algorithm has been incorporated into the newest version of superlink, which is currently the fastest genetic linkage program for exact likelihood computations in general pedigrees. We demonstrate an order of magnitude improvement in run times of likelihood computations using our new optimization algorithm, and hence enlarge the class of problems that can be handled effectively by exact computations.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {114–121},
numpages = {8},
keywords = {DAG models, bayesian networks, combinatorial optimization, genetic linkage analysis, greedy algorithms, probabilistic algorithms, superlink, treewidth},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640090,
author = {Furka, ౹rp\'{a}d},
title = {Combinatorial synthesis on macroscopic solid support units},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640090},
doi = {10.1145/640075.640090},
abstract = {The quality of our every day life is strongly affected by the new compounds and new materials introduced as a result of scientific research. In the last decade of the last century the traditional approach of research, that is, making and testing one new substance at a time, was replaced by the more efficient combinatorial methods. The philosophy staying behind these methods requires preparing a large array of compounds instead a single one. Two such methods are extensively used: the parallel synthesis and the split-mix procedure.In the parallel synthesis each compound is synthesized in a separate vessel. The number of operations used in the synthesis like the delivery of reagents and solvents is practically the same as in the conventional approach. The gain is in the reaction time. The time used for the synthesis of an array of 100 compounds essentially the same, as that needed for preparation of a single compound in the conventional approach.Following the split-mix procedure the gain is not only in the reaction time. The number of the reaction vessels in each reaction step is reduced to a minimum, that is, to the number of the executable reactions and, as a consequence, the number of operations needed in the synthesis is also substantially reduced. This explains the huge efficiency of the method. Although the split-mix synthesis leads to individual compounds, the quantity of the substance formed in a microscopic bead is very low. In order to overcome this problem while preserving the exceptionally high efficiency, the split-mix method was modified to use macroscopic solid support units instead of the microscopic beads. In one of the methods the resin was enclosed into a permeable capsule together with a transponder that carried the code of the capsule. Before each reaction step the capsules coming out from a reaction vessel were redistributed among the vessels of the next step according to their code. The other method, the string synthesis, used no label of any kind on the support units. The paper will compare the efficiency of redistribution in the two methods and describes a new version of string synthesis that makes possible to prepare selected parts of combinatorial libraries.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {122},
numpages = {1},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640091,
author = {Gionis, Aristides and Mannila, Heikki},
title = {Finding recurrent sources in sequences},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640091},
doi = {10.1145/640075.640091},
abstract = {Many genomic sequences and, more generally, (multivariate) time series display tremendous variability. However, often it is reasonable to assume that the sequence is actually generated by or assembled from a small number of sources, each of which might contribute several segments to the sequence. That is, there are h hidden sources such that the sequence can be written as a concatenation of k &gt; h pieces, each of which stems from one of the h sources. We define this (k,h)-segmentation problem and show that it is NP-hard in the general case. We give approximation algorithms achieving approximation ratios of 3 for the L1 error measure and √5 for the L2 error measure, and generalize the results to higher dimensions. We give empirical results on real (chromosome 22) and artificial data showing that the methods work well in practice.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {123–130},
numpages = {8},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640092,
author = {Greenspan, Gideon and Geiger, Dan},
title = {Model-based inference of haplotype block variation},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640092},
doi = {10.1145/640075.640092},
abstract = {The uneven recombination structure of human DNA has been highlighted by several recent studies. Knowledge of the haplotype blocks generated by this phenomenon can be applied to dramatically increase the statistical power of genetic mapping. Several criteria have already been proposed for identifying these blocks, all of which require haplotypes as input. We propose a comprehensive statistical model of haplotype block variation and show how the parameters of this model can be learned from haplotypes and/or unphased genotype data. Using real-world SNP data, we demonstrate that our approach can be used to resolve genotypes into their constituent haplotypes with greater accuracy than previously known methods.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {131–137},
numpages = {7},
keywords = {Bayesian Network, haplotype, haplotype block, haplotype resolution, linkage disequilibrium mapping, recombination hotspot},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640093,
author = {Haussler, David},
title = {Computational analysis of the human and other mammalian genomes},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640093},
doi = {10.1145/640075.640093},
abstract = {Working drafts are now available for the human, mouse and rat genomes, and other mammalian genome sequences are on the way. We discuss some of the key bioinformatic analysis problems presented by this data, including the problems of assembling the sequence, finding the genes and other functional elements, and reconstructing the evolutionary history of the genomes. Recent comparisons between the human and mouse genomes have revealed that approximately 5\% of the human genome appears to be more conserved with the orthologous regions in mouse than can be explained assuming neutral evolution. Is this the portion of the genome under selection for specific functions? How can we use comparative genomics to further pinpoint functional elements? How accurately can we reconstruct the evolutionary history of key parts of the human genome? We briefly outline some recent work (described in more detail in Adam Siepel's talk) combining hidden Markov models, used in bioinformatics to analyse DNA from a single species, with continuous time Markov models of molecular evolution, used to reconstruct evolutionary history of several species. While still a long way from answering these questions, these methods may contribute to such investigations.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {138},
numpages = {1},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640094,
author = {Heger, Andreas and Lappe, Michael and Holm, Liisa},
title = {Accurate detection of very sparse sequence motifs},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640094},
doi = {10.1145/640075.640094},
abstract = {Protein sequence alignments are more reliable the shorter the evolutionary distance. Here, we align distantly related proteins using many closely spaced intermediate sequences as stepping stones. Such transitive alignments can be generated between any two proteins in a connected set, whether they are direct or indirect sequence neighbours in the underlying library of pairwise alignments. We have implemented a greedy algorithm, MaxFlow, using a novel consistency score to estimate the relative likelihood of alternative paths of transitive alignment. In contrast to traditional profile models of amino acid preferences, MaxFlow models the probability that two positions are structurally equivalent and retains high information content across large distances in sequence space. Thus, MaxFlow is able to identify sparse and narrow active-site sequence signatures which are embedded in high-entropy sequence segments in the structure-based multiple alignment of large diverse enzyme superfamilies. In a challenging benchmark, MaxFlow yields better reliability and double coverage compared to available sequence alignment software. This promises to increase information returns from functional and structural genomics, where reliable sequence alignment is a bottleneck to transferring the functional or structural characterization of model proteins to entire protein families and superfamilies.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {139–147},
numpages = {9},
keywords = {algorithm, consistency, protein evolution, sequence alignment},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640095,
author = {Kahng, A. B. and Mandoiu, I. and Pevzner, P. and Reda, S. and Zelikovsky, A.},
title = {Engineering a scalable placement heuristic for DNA probe arrays},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640095},
doi = {10.1145/640075.640095},
abstract = {Design of DNA arrays for very large-scale immobilized polymer synthesis (VLSIPS) [8] seeks to minimize effects of unintended illumination during mask exposure steps. [9, 14] formulate this requirement as the Border Minimization Problem and give methods for placement (at array sites) and embedding (in the mask sequence) of probes in both synchronous and asynchronous regimes. These previous methods do not address several practical details of the application and, more critically, are not scalable to the O(108) probes contemplated for next-generation probe arrays. In this work, we make two main contributions:We give improved dynamic programming algorithms that perform probe embedding to minimize the number border conflicts while accounting for distance- and position-dependent border conflict weights, as well as the presence of polymorphic probes in the instance.We describe and experimentally validate the "engineering" of a scalable, high-quality asynchronous placement heuristic (which is moreover easily parallelizable) for DNA array design. Our heuristic is enabled by a novel approach for simultaneous re-placement and optimal re-embedding of an "independent set" of probes within a small window of the array..},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {148–156},
numpages = {9},
keywords = {DNA arrays, border minimization, probe placement},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640096,
author = {Kamvysselis, Manolis and Patterso, Nick and Birren, Bruce and Berger, Bonnie and Lander, Eric},
title = {Whole-genome comparative annotation and regulatory motif discovery in multiple yeast species},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640096},
doi = {10.1145/640075.640096},
abstract = {In [13] we reported the genome sequences of S. paradoxus, S. mikatae and S. bayanus and compared these three yeast species to their close relative, S. cerevisiae. Genome-wide comparative analysis allowed the identification of functionally important sequences, both coding and non-coding. In this companion paper we describe the mathematical and algorithmic results underpinning the analysis of these genomes.We developed statistical methods for the systematic de-novo identification of regulatory motifs. Without making use of co-regulated gene sets, we discovered virtually all previously known DNA regulatory motifs as well as several noteworthy novel motifs. With the additional use of gene ontology information, expression clusters and transcription factor binding profiles, we assigned candidate functions to the novel motifs discovered.Our results demonstrate that entirely automatic genome-wide annotation, gene validation, and discovery of regulatory motifs is possible. Our findings are validated by the extensive experimental knowledge in yeast, confirming their applicability to other genomes.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {157–166},
numpages = {10},
keywords = {comparative genomics, computational biology, genome annotation, regulatory motif discovery},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640097,
author = {Krishnapuram, Balaji and Carin, Lawrence and Hartemink, Alexander J.},
title = {Joint classifier and feature optimization for cancer diagnosis using gene expression data},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640097},
doi = {10.1145/640075.640097},
abstract = {Recent research has demonstrated quite convincingly that accurate cancer diagnosis can be achieved by constructing classifiers that are designed to compare the gene expression profile of a tissue of unknown cancer status to a database of stored expression profiles from tissues of known cancer status. This paper introduces the JCFO, a novel algorithm that uses a sparse Bayesian approach to jointly identify both the optimal nonlinear classifier for diagnosis and the optimal set of genes on which to base that diagnosis. We show that the diagnostic classification accuracy of the proposed algorithm is superior to a number of current state-of-the-art methods in a full leave-one-out cross-validation study of two widely used benchmark datasets. In addition to its superior classification accuracy, the algorithm is designed to automatically identify a small subset of genes (typically around twenty in our experiments) that are capable of providing complete discriminatory information for diagnosis. Focusing attention on a small subset of genes is not only useful because it produces a classifier with good generalization capacity, but also because this set of genes may provide insights into the mechanisms responsible for the disease itself. A number of the genes identified by the JCFO in our experiments are already in use as clinical markers for cancer diagnosis; some of the remaining genes may be excellent candidates for further clinical investigation. If it is possible to identify a small set of genes that is indeed capable of providing complete discrimination, inexpensive diagnostic assays might be widely deployable in clinical settings.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {167–175},
numpages = {9},
keywords = {JCFO, RVM, SVM, classication, disease diagnosis, feature selection, joint optimization, sparse bayesian methods},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640098,
author = {Langmead, Christopher James and Yan, Anthony and Lilien, Ryan and Wang, Lincong and Donald, Bruce Randall},
title = {Large a polynomial-time nuclear vector replacement algorithm for automated NMR resonance assignments},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640098},
doi = {10.1145/640075.640098},
abstract = {High-throughput NMR structural biology can play an important role in structural genomics. We report an automated procedure for high-throughput NMR resonance assignment for a protein of known structure, or of an homologous structure. These assignments are a prerequisite for probing protein-protein interactions, protein-ligand binding, and dynamics by NMR. Assignments are also the starting point for structure determination and refinement. A new algorithm, called Nuclear Vector Replacement (NVR) is introduced to compute assignments that optimally correlate experimentally-measured NH residual dipolar couplings (RDCs) to a given a priori whole-protein 3D structural model. The algorithm requires only uniform 15N-labelling of the protein, and processes unassigned HN-15N HSQC spectra, HN-15N RDCs, and sparse HN-HN NOE's dNNs), all of which can be acquired in a fraction of the time needed to record the traditional suite of experiments used to perform resonance assignments. NVR runs in minutes and efficiently assigns the (HN,15N) backbone resonances as well as the dNNs of the 3D nfif-NOESY spectrum, in O(n3) time. The algorithm is demonstrated on NMR data from a 76-residue protein, human ubiquitin, matched to four structures, including one mutant (homolog), determined either by X-ray crystallography or by different NMR experiments (without RDCs). NVR achieves an average assignment accuracy of over 90\%. We further demonstrate the feasibility of our algorithm for different and larger proteins, using NMR data for hen lysozyme (129 residues, 98\% accuracy) and streptococcal protein G (56 residues, 95\% accuracy), matched to a variety of 3D structural models. Finally, we extend NVR to a second application, 3D structural homology detection, and demonstrate that NVR is able to identify structural homologies between proteins with remote amino acid sequences using a database of structural models.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {176–187},
numpages = {12},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640099,
author = {Lesh, Neal and Mitzenmacher, Michael and Whitesides, Sue},
title = {A complete and effective move set for simplified protein folding},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640099},
doi = {10.1145/640075.640099},
abstract = {We present new lowest energy configurations for several large benchmark problems for the two-dimensional hydrophobic-hydrophilic model. We found these solutions with a generic implementation of tabu search using an apparently novel set of transformations that we call pull moves. Our experiments show that our algorithm can find these best solutions in 3 to 14 hours, on average. Pull moves appear quite effective and may also be useful for other local search algorithms for the problem. Additionally, we prove that pull moves are complete; that is, any pair of valid configurations are mutually reachable through a sequence of pull moves. Our implementation was developed with the Human-Guided Search (HuGS) middleware, which allows rapid development of interactive optimization systems.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {188–195},
numpages = {8},
keywords = {tabu search, protein folding, local moves},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640100,
author = {Lesk, Arthur M.},
title = {Invited: Prediction of protein function},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640100},
doi = {10.1145/640075.640100},
abstract = {A genome sequence embodies the potential life of an organism, but implementation of genetic information depends on the functions of the proteins that it encodes. Many proteins of known sequence and even of known structure present challenges to understanding their function. In some cases genes responsible for diseases have been identified but their specific functions are unknown. Annotation of a genome involves assignment of functions to gene products, in most cases on the basis of amino acid sequence alone, in the absence of experimental information. Protein structures from structural genomics projects are invaluable for function assignment. Nevertheless, prediction of protein function remains a difficult problem. Some methods provide reasonable guesses, but no method is foolproof. Moreover, even when it is possible to ascribe a particular function to a gene product, the protein may have multiple functions. An underlying problem has been that function is in many cases an ill-defined concept.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {196},
numpages = {1},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640101,
author = {Li, Jing and Jiang, Tao},
title = {Efficient rule-based haplotyping algorithms for pedigree data},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640101},
doi = {10.1145/640075.640101},
abstract = {We study haplotype reconstruction under the Mendelian law of inheritance and the minimum recombination principleon pedigree data. We prove that the problem of finding a mini-mum-recombinant haplotype configuration (MRHC) is in general NP-hard. This is the first complexity result concerning the problem to our knowledge. An iterative algorithm based on blocks of consecutive resolved marker loci (called block-extension) is proposed. It is very efficient and can be used for large pedigrees with a large number of markers, especially for those data sets requiring few recombinants (or recombination events). A polynomial-time exact algorithm for haplotype reconstruction without recombinants is also presented. This algorithm first identifies all the necessary constraints based on the Mendelian law and the zero recombinant assumption, and represents them using a system of linear equations over the cyclic group Z2. By using a simple method based on Gaussian elimination, we could obtain all possible feasible haplotype configurations. We have tested the block-extension algorithm on simulated data generated on three pedigree structures. The results show that the algorithm performs very well on both multi-allelic and biallelic data, especially when the number of recombinants is small.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {197–206},
numpages = {10},
keywords = {time complexity, recombination, pedigree analysis, haplotyping, algorithm, SNP, Gaussian elimination over Z2},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640102,
author = {Li, Lei and Kim, Jong Hyun and Waterman, Michael S.},
title = {Haplotype reconstruction from SNP alignment},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640102},
doi = {10.1145/640075.640102},
abstract = {In this paper we describe a method for statistical reconstruction of haplotypes from a set of aligned SNP fragments. We consider the case of a pair of homologous human chromosomes, one from the mother and the other from the father. After fragment assembly and SNP detection, we wish to reconstruct the two haplotypes of the parents. Given a set of SNP sites inferred from the assembly alignment, we wish to divide the fragment set into two subsets, each of which represents one chromosome. Our method is based on a statistical model of sequencing errors, compositional information and haplotype memberships.We calculate probabilities of different haplotypes conditional on the alignment. Due to computational complexity, we first determine phases for neighboring SNPs. Then we connect them and construct haplotype segments. Also we compute the accuracy or confidence of the reconstructed haplotypes. We discuss other issues such as alternative methods, parameter estimation, computational efficiency, and relaxation of assumptions.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {207–216},
numpages = {10},
keywords = {shotgun sequencing, haplotype, genotype, confidence, alignment, SNP},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640103,
author = {Li, Wentian and Grosse, Ivo},
title = {Gene selection criterion for discriminant microarray data analysis based on extreme value distributions},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640103},
doi = {10.1145/640075.640103},
abstract = {An important issue commonly encountered in the analysis of microarray data is to decide which and how many genes should be selected for further studies. For discriminant microarray data analyses based on statistical models, such as the logistic regression model, this gene selection can be accomplished by a comparison of the maximum likelihood of the model given the real data, L(D|M), and the expected maximum likelihood of the model given an ensemble of surrogate data, L(D0|M). Typically, the computational burden for obtaining L(D0|M) is immense, often exceeding the limits of available resources by orders of magnitude. Here, we propose an approach that circumvents such heavy computations by mapping the simulation problem to an extreme value problem, which can be easily solved by numerical simulation. We choose three classification problems from two publicly available microarray datasets to illustrate that approach.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {217–223},
numpages = {7},
keywords = {microarray, logistic regression, extreme values, classification},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640104,
author = {Nagarajan, Niranjan and Yona, Golan},
title = {A multi-expert system for the automatic detection of protein domains from sequence information},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640104},
doi = {10.1145/640075.640104},
abstract = {We describe a novel method for detecting the domain structure of a protein from sequence information alone. The method is based on analyzing multiple sequence alignments that are derived from a database search. Multiple measures are defined to quantify the domain information content of each position along the sequence, and are combined into a single predictor using a neural network. The output is further smoothed and post-processed using a probabilistic model to predict the most likely transition or boundary positions between domains. The method was assessed using the domain definitions in SCOP for proteins of known structures and was compared to several other existing methods. Our method improves significantly over the best method available, the semi-manual PFam domain database, while being fully automatic. Our method can also be used to verify domain partitions based on structural data. Few examples of predicted domain definitions and alternative partitions, as suggested by our method, are also discussed.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {224–234},
numpages = {11},
keywords = {protein domains, domain prediction, domain boundaries, SCOP},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640105,
author = {N\"{u}sslein-Volhard, Christiane},
title = {Invited},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640105},
doi = {10.1145/640075.640105},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {235},
numpages = {1},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640106,
author = {Oesterhelt, Dieter},
title = {Invited: Biology and bioinformatics of halophilism},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640106},
doi = {10.1145/640075.640106},
abstract = {Halophilism is a biological response to habitats with extreme salt concentrations. We sequenced the genomes of two extremely halophilic archaea for detailed analysis of life style and adaptation to extreme environments. A central focus are studies on bioenergetics and signal transduction. The genome has a size of 2 mb pairs and automatted followed by manual annotation yielded about 2300 open reading frames as a basis for the construction of DNA arrays. Exhaustive MALDI TOF mass spectroscopic analysis of the proteins allowed the identification of a substantial part of the cellular inventory, but membrane proteins, for practical and theoretical reasons will be subject to other mass spectroscopic techniques.Results of manual annotation, literature search results of chemical and genetic experimentation and the increasing data sets from proteomic and transcriptomic analysis are stored in a data bank system, Halolex. All information collected is classified in validation levels and manual input avoids the propagation of existing data base errors. Halolex also collects all available biological facts about Halo-bacterium salinarum comprising data on its microbiology, physiology, biochemistry and genetics.Data nets on modules of the cell are created by experimentation which allow to model and simulate parts of the cellular events. The ultimate goal is the prediction of experimental results on the basis of such models. We started to collect quantitative data on two areas which serve as first modules, which is bioenergetics and signal transduction. The bioenergetics of the halophilic cell comprise fermentation (arginine), aerobic (oxygen) or anaerobic (DMSO) respiration and photosynthesis (bacteriorhododopsin). Experimental observables are the rate of respiration, the efficiency of photon conversion, potassium accumulation and ATP production in the cell.Signal transduction in H.salinarum has two targets: the flagellar motor for motility and the genome for regulation of transcription. The flagellar motor rotates clockwise and counterclockwise in the unstimulated case causing an unbiassed walk without a change of the spatial coordinates of the cell. External signals are received by a total of 18 different receptors which transduce the chemical and physical stimuli and regulate the phosphorylation cascade of a two-component system ending at the flagellar motor. This results in a biassed walk ultimately leading to a movement towards positive sources and away from negative sources of stimuli. The signal transduction chain involving reception, amplification, integration and adaptation is known for its molecular components and in the case of phototaxis can be quantitatively described by an equation linking the input (photon exposure) to the output (response time) of the flagellar motor. Numerical simulation in cooperation with the groups of E. Gilles and W. Marwan aim now to model the entire phototactic system. This will serve as a first module to be extended to more of the 18 receptor-mediated signal transduction chains, such as chemotaxis or proton motive force taxis.The second target is the genome and here a link between the bioenergetic module and the signal transduction network occurs. While 13 more histidine kinases and three further response regulators serve as a basis for the variety of transcriptional regulatory responses of the cell to changing environments three signals, which are so important for bioenergetics, i.e. light, oxygen and arginine, are targeted not only to the flagellar motor but also to the genome to up and down regulate the expression of the corresponding bioenergetic systems. It is our aim to collect quantitative data on this module and subject it to numerical simulation.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {236},
numpages = {1},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640107,
author = {Pe'er, Itsik and Beckmann, Jacques S.},
title = {Resolution of haplotypes and haplotype frequencies from SNP genotypes of pooled samples},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640107},
doi = {10.1145/640075.640107},
abstract = {Recent efforts to characterize genetic variation indicate that humans share large chromosomal blocks, along which little to no recombination is observable. Thus, on a segment-by-segment basis, only a handful of haplotypes account for most human genotypes. Currently, the challenge of registering haplotypes and their frequencies is met by genotyping individuals one by one, a process which is overall resource intensive. Instead, we propose utilizing the ability of current genotyping technologies to pool DNA samples and output allele frequencies of SNP markers. We enable inference of haplotypes and haplotype frequencies from such pooled data, by novel computational methods. This strategy harnesses the economics of pooling for the task of haplotyping, potentially offering a 10-20-fold saving in genotyping reactions performed.We show that a small number of pools can be used to accurately and cost-effectively reconstruct a haplotype block and determine haplotype frequencies.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {237–246},
numpages = {10},
keywords = {pooling, lattices, clustering, SNP genotypes, Haplotype},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640108,
author = {Pevzner, Pavel and Tesler, Glenn},
title = {Transforming men into mice: the Nadeau-Taylor chromosomal breakage model revisited},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640108},
doi = {10.1145/640075.640108},
abstract = {Although analysis of genome rearrangements was pioneered by Dobzhansky and Sturtevant 65 years ago, we still know very little about the rearrangement events that produced the existing varieties of genomic architectures. The genomic sequences of human and mouse provide evidence for a larger number of rearrangements than previously thought and shed some light on previously unknown features of mammalian evolution. In particular, they reveal extensive re-use of breakpoints from the same relatively short regions. Our analysis implies the existence of a large number of very short "hidden" synteny blocks that were invisible in comparative mapping data and were not taken into account in previous studies of chromosome evolution. These blocks are defined by closely located breakpoints and are often hard to detect. Our result is in conflict with the widely accepted random breakage model of chromosomal evolution. We suggest a new "fragile breakage" model of chromosome evolution that postulates that breakpoints are chosen from relatively short fragile regions that have much higher propensity for rearrangements than the rest of the genome.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {247–256},
numpages = {10},
keywords = {genome rearrangements, evolution, breakpoint re-use},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640109,
author = {Roskin, Krishna M. and Diekhans, Mark and Haussler, David},
title = {Scoring two-species local alignments to try to statistically separate neutrally evolving from selected DNA segments},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640109},
doi = {10.1145/640075.640109},
abstract = {We construct several score functions for use in locating unusually conserved regions in a genome-wide search of aligned DNA from two species. We test these functions on regions of the human genome aligned to the mouse genome. These score functions are derived from properties of neutrally evolving sites on the mouse and human genome, and can be adjusted to the local background rate of conservation. The aim of these functions is to try to identify regions of the human genome that are conserved by evolutionary selection, because they have an important function, rather than by chance. We use them to get a very rough estimate of the amount of DNA in the human genome that is under selection.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {257–266},
numpages = {10},
keywords = {neutral evolution, mutual information, mouse-human alignments, fraction of human genome under selection, evolutionary models, dinucleotide dependence, context-dependent base substitutions, comparative genomics, ancestral repeat, CpG effect},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640110,
author = {Schwarzer, Fabian and Lotan, Itay},
title = {Approximation of protein structure for fast similarity measures},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640110},
doi = {10.1145/640075.640110},
abstract = {It is shown that structural similarity between proteins can be decided well with much less information than what is used in common similarity measures. The full Cα representation contains redundant information because of the inherent chain topology of proteins and a limit on their compactness due to excluded volume. A wavelet analysis on random chains and proteins justifies approximating subchains by their centers of mass. For not too compact chain-like structures in general, and proteins in particular, similarity measures that use this approximation are highly correlated to the exact similarity measures and are therefore useful, e.g., as fast filters. Experimental results with such simplified similarity measures in two applications, nearest neighbor search and automatic structural classification show a significant speed up.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {267–276},
numpages = {10},
keywords = {similarity measures, protein structure, nearest-neighbor search, approximation of structure},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640111,
author = {Siepel, Adam and Haussler, David},
title = {Combining phylogenetic and hidden Markov models in biosequence analysis},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640111},
doi = {10.1145/640075.640111},
abstract = {A few models have appeared in recent years that consider not only the way substitutions occur through evolutionary history at each site of a genome, but also the way the process changes from one site to the next. These models combine phylogenetic models of molecular evolution, which apply to individual sites, and hidden Markov models, which allow for changes from site to site. Besides improving the realism of ordinary phylogenetic models, they are potentially very powerful tools for inference and prediction---for gene finding, for example, or prediction of secondary structure. In this paper, we review progress on combined phylogenetic and hidden Markov models and present some extensions to previous work. Our main result is a simple and efficient method for accommodating higher-order states in the HMM, which allows for context-sensitive models of substitution---that is, models that consider the effects of neighboring bases on the pattern of substitution. We present experimental results indicating that higher-order states, autocorrelated rates, and multiple functional categories all lead to significant improvements in the fit of a combined phylogenetic and hidden Markov model, with the effect of higher-order states being particularly pronounced.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {277–286},
numpages = {10},
keywords = {maximum likelihood, context-sensitive substitution},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640112,
author = {Speed, Terry},
title = {Invited: low-level analyses of microarray data},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640112},
doi = {10.1145/640075.640112},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {287},
numpages = {1},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640113,
author = {Stefansson, Kari},
title = {Invited},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640113},
doi = {10.1145/640075.640113},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {288},
numpages = {1},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640114,
author = {Syed, Umar and Yona, Golan},
title = {Using a mixture of probabilistic decision trees for direct prediction of protein function},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640114},
doi = {10.1145/640075.640114},
abstract = {We study the direct relationship between basic protein properties and their function. Our goal is to develop a new tool for functional prediction that can be used to complement and support other techniques based on sequence or structure information. In order to define this new measure of similarity between proteins we collected a set of 453 features and properties that characterize proteins and are believed to be correlated and related to structural and functional aspects of proteins. Among these properties are the composition and fraction of different groups of amino acids, predicted secondary structure content, molecular weight, average hydrophobicity, isoelectric point and others, as well as a set of properties that are extracted from database records of known protein sequences, such as subcellular location, tissue specificity, and others.We introduce the mixture model of probabilistic decision trees to learn the set of potentially complex relationships between features and function. To study these correlations, trees are created and tested on the Pfam sequence-based classification of proteins and the EC classification of enzyme families. The model is very effective in learning highly diverged protein families or families that are not defined based on sequence. The resulting tree structure indicates the properties that are strongly correlated with structural and functional aspects of protein families, and can be used to suggest a concise definition of a protein family.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {289–300},
numpages = {12},
keywords = {sequence-function relationships, functional prediction, decision trees},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640115,
author = {Tanay, Amos and Shamir, Ron},
title = {Modeling transcription programs: inferring binding site activity and dose-response model optimization},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640115},
doi = {10.1145/640075.640115},
abstract = {The modeling of transcription regulation programs is a major focus of today's biology. The challenge is to utilize diverse high-throughput data (gene expression, promoter binding site localization assays, protein expression) in order to infer the mechanistic models of transcription control. We propose a new model which integrates transcription factor-gene affinities, protein abundance and gene expression levels. Transcription factor binding site activity is represented by a dose-affinity-response function, and regulation is assumed to be a combinatorial function of the activities of the binding sites in the gene's promoter sites.We develop algorithms that infer the model given complete data and give a fast polynomial time algorithm under reasonable assumptions. We also show how to assess initial values of missing data (notably protein abundance) using a novel framework for active motif detection, which may be of independent interest. We test the various components of the framework on gene expression data related to carbohydrate metabolism in yeast. The results demonstrate the high specificity and sensitivity of the approach and its advantages over extant motif activity detection methods. We are also able to predict new active motifs in the galactose pathway.A key feature of our method is the global approach to transcription factor activity and to the relation between this activity and promoter signals. We use dozens of genes, with many different promoter signals and expression levels in order to draw conclusions on the function of a single transcription factor. This provides us the robustness necessary in order to overcome the considerable level of noise in the data.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {301–310},
numpages = {10},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640116,
author = {Trifonov, Edward N.},
title = {Invited: poetry and prose of the sequences},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640116},
doi = {10.1145/640075.640116},
abstract = {Nucleotide and amino acid sequences carry many different codes, patterns recognized by numerous different reading devices. To name a few: THE triplet code, gene splicing code, translation framing code, chromatin code. One very basic property of the codes is their overlapping, so that one and the same character in given position of given sequence is involved simultaneously in several different messages. Progress in elucidation of the chromatin code will be reported. A newly discovered proteomic code of protein sequences will be discussed. It is based on the universal loop fold structure of globular proteins. The abundant standard size closed loops, 25-35 amino-acid residues, apparently, represent descendants of the ancestral closed loops, each with its specific sequence and structure. The evolution of the triplet code and early evolution of proteins will be outlined.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {311},
numpages = {1},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640117,
author = {Yeang, Chen-Hsiang and Jaakkola, Tommi},
title = {Physical network models and multi-source data integration},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640117},
doi = {10.1145/640075.640117},
abstract = {We develop a new framework for inferring models of transcriptional regulation. The models in this approach, which we call physical models, are constructed on the basis of verifiable molecular attributes of the underlying biological system. The attributes include, for example, the existence of protein-protein and protein-DNA interactions in gene regulatory processes, the directionality of signal transduction in protein-protein interactions, as well as the signs of the immediate effects of these interactions (e.g., whether an upstream gen activates or represses the downstream genes). Each attribute is included as a variable in the model, and the variables define a collection of annotated random graphs. Possible configurations of these variables (realizations of the underlying biological system) are constrained by the available data sources. Some of the data sources such as factor-binding data (location data) involve measurements that are directly tied to the variables in the model. Other sources such as gene knock-outs are functional in nature and provide only indirect evidence about the (physical) variables. We associate each knock-out effect in the deletion mutant data with a set of causal paths (molecular cascades) that could in principle explain the effect, resulting in aggregate constraints about the physical variables in the model. The most likely setting of all the variables is found by the max-product algorithm. By testing our approach on datasets related to the pheromone response pathway in S. cerevisiae, we demonstrate that the resulting transcriptional models are consistent with previous studies about the pathway. Moreover, we show that the approach is capable of predicting gene knock-out effects with high degree of accuracy in a cross-validation setting. The method also implicates likely molecular cascades responsible for each observed knock-out effect. The inference results are robust against variations in the model parameters. We can extend the approach to include other data sources such as time course expression profiles. We also discuss coordinated regulation and the use of automated experiment design},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {312–321},
numpages = {10},
keywords = {regulatory networks, physical models, data fusion},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640118,
author = {Yeo, Gene and Burge, Christopher B.},
title = {Maximum entropy modeling of short sequence motifs with applications to RNA splicing signals},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640118},
doi = {10.1145/640075.640118},
abstract = {We propose a framework for modeling sequence motifs based on the Maximum Entropy principle (MEP).We recommend approximating short sequence motif distributions with the Maximum Entropy Distribution (MED) consistent with low-order marginal constraints estimated from available data, which may include dependencies between non-adjacent as well as adjacent positions.Finally, we suggest mechanistically-motivated ways of comparing models.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {322–331},
numpages = {10},
keywords = {splice sites, regulatory elements, non-neighboring dependencies, molecular sequence analysis, maximum entropy, maximal dependence decomposition, Markov models},
location = {Berlin, Germany},
series = {RECOMB '03}
}

@inproceedings{10.1145/640075.640119,
author = {Zhang, Kui and Sun, Fengzhu and Waterman, Michael S. and Chen, Ting},
title = {Dynamic programming algorithms for haplotype block partitioning: applications to human chromosome 21 haplotype data},
year = {2003},
isbn = {1581136358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/640075.640119},
doi = {10.1145/640075.640119},
abstract = {Recent studies have shown that the human genome has a haplotype block structure such that it can be divided into discrete blocks of limited haplotype diversity. Patil et al. [6] and Zhang et al. [12] developed algorithms to partition haplotypes into blocks with minimum number of tag SNPs for the entire chromosome. However, it is not clear how to partition haplotypes into blocks with restricted number of SNPs when only limited resources are available. In this paper, we first formulated this problem as finding a block partition with a fixed number of tag SNPs that can cover the maximal percentage of a genome. Then we solved it by two dynamic programming algorithms, which are fairly flexible to take into account the knowledge of functional polymorphism. We applied our algorithms to the published SNP data of human chromosome 21 combining with the functional information of these SNPs and demonstrated the effectiveness of them. Statistical investigation of the relationship between the starting points of a block partition and the coding and non-coding regions illuminated that the SNPs at these starting points are not significantly enriched in coding regions. We also developed an efficient algorithm to find all possible long local maximal haplotypes across a subset of samples. After applying this algorithm to the human chromosome 21 haplotype data, we found that samples with long local haplotypes are not necessarily globally similar.},
booktitle = {Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology},
pages = {332–340},
numpages = {9},
keywords = {tag SNPs, local maximal haplotypes, haplotype block, dynamic programming, clustering},
location = {Berlin, Germany},
series = {RECOMB '03}
}

